{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 7: Evaluation Optimization\n",
    "\n",
    "In this project, we will develop a question-answering system. Please note that **usage of the `STANDARD_NC8AS_T4_V3` GPU compute on Azure ML Studio is required to pass the local tests, starting from Question 2**. Similar to Project 6, however, you may find it useful to test your code on an external environment (your local computer or Google CoLab) to make sure there are no runtime errors first.\n",
    "\n",
    "## Some words of caution\n",
    "Based on past semesters, this has been the most difficult project in the course. You will be working with computationally heavy models, and the local test for each question may take from 3-10 minutes to run. In addition, Question 8 (Fine-tuning BERT) will take one hour to run. Given these challenges, we have tried to be as thorough as possible in the question descriptions. Please read them carefully --  **spending 5 minutes to double check the instructions can sometimes save you hours of debugging**. In addition, plan your implementation carefully before writing any code, and always start testing on small subsets of the data.\n",
    "\n",
    "For long-running tasks, a great way to track your code's progress is through the `tqdm` library (see the [TQDM primer](https://nbviewer.jupyter.org/url/clouddatascience.blob.core.windows.net/primers/tqdm-primer/tqdm_primer.ipynb) for more details), which we have imported below. Whenever you use Pandas' `.apply` or `.map` method, replace them with `.progress_apply` and `.progress_map` to see the progress bar.\n",
    "\n",
    "Also make sure to monitor your Azure budget carefully; you will not be able to finish this project and the final exam if you run out of budget. Note that **your remaining budget is in the Education tab on Azure, not the Cost Monitoring tab**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package import\n",
    "Make sure to set the kernel as Python 3.8 - AzureML before starting your work. You can ignore the warning messages about tensorflow when running the import cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 00:17:26.890856: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-22 00:17:36.670353: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-22 00:17:36.670444: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-22 00:17:36.670451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-04-22 00:17:42.072954: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-04-22 00:17:42.073023: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import re, os, json, pickle, ast, time, random, requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"stopwords\", quiet = True)\n",
    "nltk.download(\"wordnet\", quiet = True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet = True)\n",
    "nltk.download(\"punkt\", quiet = True)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "GLOBAL_SEED = 1\n",
    " \n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "GLOBAL_WORKER_ID = None\n",
    "def _init_fn(worker_id):\n",
    "    global GLOBAL_WORKER_ID\n",
    "    GLOBAL_WORKER_ID = worker_id\n",
    "    set_seed(GLOBAL_SEED + worker_id)\n",
    "\n",
    "set_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equal(actual, expected):\n",
    "    assert actual == expected, actual\n",
    "\n",
    "def check_approx(actual, expected):\n",
    "    assert np.allclose(actual, expected), actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Project Overview\n",
    "We will train our NLP models on the [Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/), a reading comprehension dataset with more than 100,000 questions. SQuAD was one of the first with a public leaderboard and thus was able to garner a large amount of research result and publicity towards itself. Questions in the dataset can be answered from the context that accompanies them, without requiring any domain-specific knowledge; thus they belong to the class of *single-hop* question answering problem.\n",
    "\n",
    "Here's an example of what our model will do: given a *question*,\n",
    "```\n",
    "When did Beyonce start becoming popular?\n",
    "```\n",
    "and a block of text containing the answer, called the *context*,\n",
    "```\n",
    "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
    "```\n",
    "our model will be able to extract the answer from this context, which is\n",
    "```\n",
    "in the late 1990s\n",
    "```\n",
    "\n",
    "We will start by evaluating some simple models that only identify the sentence which contains the answer (i.e., the *answer sentence*) within the context. Then we'll move to a state-of-the-art model called BERT that can also identify the exact answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the dataset -- note that the following cell should have the tag `excluded_from_script`, since the autograder will use a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context_paragraph</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "      <th>context_sentences</th>\n",
       "      <th>answer_sent_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>269</td>\n",
       "      <td>286</td>\n",
       "      <td>[Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>207</td>\n",
       "      <td>226</td>\n",
       "      <td>[Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>2003</td>\n",
       "      <td>526</td>\n",
       "      <td>530</td>\n",
       "      <td>[Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>166</td>\n",
       "      <td>180</td>\n",
       "      <td>[Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>276</td>\n",
       "      <td>286</td>\n",
       "      <td>[Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0           When did Beyonce start becoming popular?   \n",
       "1  What areas did Beyonce compete in when she was...   \n",
       "2  When did Beyonce leave Destiny's Child and bec...   \n",
       "3      In what city and state did Beyonce  grow up?    \n",
       "4         In which decade did Beyonce become famous?   \n",
       "\n",
       "                                   context_paragraph               answer  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...    in the late 1990s   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  singing and dancing   \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...                 2003   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...       Houston, Texas   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...           late 1990s   \n",
       "\n",
       "   answer_start  answer_end  \\\n",
       "0           269         286   \n",
       "1           207         226   \n",
       "2           526         530   \n",
       "3           166         180   \n",
       "4           276         286   \n",
       "\n",
       "                                   context_sentences  answer_sent_index  \n",
       "0  [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...                  1  \n",
       "1  [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...                  1  \n",
       "2  [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...                  3  \n",
       "3  [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...                  1  \n",
       "4  [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ ...                  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad = pd.read_csv(\n",
    "    \"cleaned_squad_data.csv\",\n",
    "    dtype = { \n",
    "        \"question\" : str,\n",
    "        \"context_paragraph\" : str,\n",
    "        \"answer\" : str,\n",
    "        \"answer_start\" : int,\n",
    "        \"answer_end\" : int,\n",
    "        \"answer_sent_index\" : int,\n",
    "        \"tokenized_context\" : str\n",
    "    },\n",
    "    converters = {\"context_sentences\" : ast.literal_eval}\n",
    ")\n",
    "\n",
    "df_squad.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the cell contents are truncated, let's print out and examine one row in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'In what city and state did Beyonce  grow up? ',\n",
       " 'context_paragraph': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'answer': 'Houston, Texas',\n",
       " 'answer_start': 166,\n",
       " 'answer_end': 180,\n",
       " 'context_sentences': ['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress.',\n",
       "  \"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\",\n",
       "  \"Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time.\",\n",
       "  'Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'],\n",
       " 'answer_sent_index': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_squad.iloc[3].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get a better understanding of what each column means:\n",
    "* `question` is the question text.\n",
    "* `context_paragraph` is the paragraph of text that contains the answer, which our model extracts from.\n",
    "* `answer` is the ground-truth answer to the given question.\n",
    "* `answer_start` and `answer_end` are the indexes of the first and last character of `answer` within `context_paragraph`. In other words, `context_paragraph[answer_start:answer_end]` yields `answer`. Note that `answer_end` is not inclusive.\n",
    "* `context_sentences` is the list of sentences in `context_paragraph`.\n",
    "* `answer_sent_index` is the ground-truth index of the answer sentence within `context_sentences` (indexing starts from 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several techniques to build a question-answering model from this dataset, which we will introduce in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Unsupervised Models\n",
    "In this section, we will implement three unsupervised learning models to identify the sentence that contains the answer to a given question. Here \"unsupervised\" means that we will not make use of the ground truth answer provided in the dataset (i.e., the columns `text`, `answer_start`, and `answer_end`). Instead, the sentence identification will be based only on some pre-defined heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Jaccard Overlap\n",
    "The Jaccard overlap of two given sets $A$ and $B$ measures the similarity between them and is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "J(A, B) = \\begin{cases}\n",
    "    \\frac{|A \\cap B|}{|A \\cup B|} & \\text{ if $A \\ne \\emptyset$ or $B \\ne \\emptyset$ } \\\\\n",
    "    1 & \\text { otherwise }\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Given a question and a list of context sentences, we can identify the answer sentence using Jaccard overlap as follows:\n",
    "1. Construct the set of words that are in the input question; we will call this set $Q$.\n",
    "1. Construct the sets of words that are in each context sentence; we will call these sets $S_1, S_2, \\ldots, S_m$. Here $S_i$ is the set of words in the $i$-th context sentence.\n",
    "1. Return the index of the context sentence whose Jaccard overlap with the input question is largest; this is our predicted answer sentence: \n",
    "\n",
    "$$\\hat y = \\underset{1 \\le i \\le m}{\\operatorname{argmax}} J(Q, S_i).$$\n",
    "\n",
    "Implement the function `get_jaccard_prediction` that performs the above steps on the dataset `df_squad`. For every row, it stores the predicted answer sentence index in a new column `\"jaccard_prediction\"`, and the corresponding largest Jaccard overlap value in a new column `\"jaccard_value\"`.\n",
    "\n",
    "**Notes**:\n",
    "* Our math notations use 1-based indexing, but in your implementation the indexes start from 0. In other words, if the first sentence in the context paragraph is the predicted answer sentence, you should return 0.\n",
    "* If multiple context sentences have the same (largest) Jaccard overlap with the question, return the smallest sentence index.\n",
    "* To build the set of words from a sentence, you should first tokenize the sentence with `nltk`, and then turn the resulting list of tokens into a set.\n",
    "* You do not need to perform any rounding on the distance values.\n",
    "* Refer to the [Pandas primer](https://nbviewer.jupyter.org/url/clouddatascience.blob.core.windows.net/primers/pandas-primer/pandas_primer.ipynb) on how to vectorize a row-wise dataframe operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_prediction(df_squad):\n",
    "    \"\"\"\n",
    "    Identify the answer sentence as one that has the largest Jaccard overlap with the input question.\n",
    "    \n",
    "    args:\n",
    "        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n",
    "        \n",
    "    returns:\n",
    "        pd.DataFrame : the input dataframe with two additional columns, \"jaccard_prediction\" and \"jaccard_value\"\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for index, row in df_squad.iterrows():\n",
    "        question_tokens = set(word_tokenize(row['question']))\n",
    "        sentences_tokens = [set(word_tokenize(sentence)) for sentence in row['context_sentences']]\n",
    "        jaccard_scores = []\n",
    "        \n",
    "        for tokens in sentences_tokens:\n",
    "            intersection = question_tokens.intersection(tokens)\n",
    "            union = question_tokens.union(tokens)\n",
    "            jaccard_score = len(intersection) / len(union) if union else 0\n",
    "            jaccard_scores.append(jaccard_score)\n",
    "            \n",
    "        max_jaccard_value = max(jaccard_scores)\n",
    "        max_jaccard_index = jaccard_scores.index(max_jaccard_value)\n",
    "        \n",
    "        results.append([max_jaccard_index, max_jaccard_value])\n",
    "        \n",
    "    result_df = pd.DataFrame(results, columns=['jaccard_prediction', 'jaccard_value'])\n",
    "    df_squad[['jaccard_prediction', 'jaccard_value']] = result_df\n",
    "\n",
    "    return df_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_get_jaccard_prediction():\n",
    "    \"\"\"Test on the first 10 rows\"\"\"\n",
    "    df_jaccard = get_jaccard_prediction(df_squad.head(10).copy())\n",
    "    \n",
    "    check_equal(df_jaccard.shape, (10, 9))\n",
    "    \n",
    "    check_approx(df_jaccard[\"jaccard_value\"].tolist(),[\n",
    "        0.0, 0.046511627906976744, 0.15, 0.03225806451612903, 0.02040816326530612,\n",
    "        0.18421052631578946, 0.10869565217391304, 0.12, 0.05263157894736842, 0.10256410256410256\n",
    "    ])\n",
    "    \n",
    "    check_equal(df_jaccard[\"jaccard_prediction\"].tolist(), [0, 1, 1, 0, 3, 1, 3, 2, 1, 1])\n",
    "    \n",
    "    jaccard_accuracy = (df_jaccard[\"jaccard_prediction\"] == df_jaccard[\"answer_sent_index\"]).values.mean()\n",
    "    check_equal(jaccard_accuracy, 0.6)\n",
    "    \n",
    "    \n",
    "    \"\"\"Test on the entire dataset\"\"\"\n",
    "    df_jaccard = get_jaccard_prediction(df_squad.copy())\n",
    "    \n",
    "    check_equal(df_jaccard.shape, (86821, 9))\n",
    "    \n",
    "    check_approx(df_jaccard.tail(10)[\"jaccard_value\"].tolist(), [\n",
    "        0.11428571428571428, 0.17073170731707318, 0.08333333333333333, 0.10526315789473684, 0.125,\n",
    "        0.10714285714285714, 0.04, 0.07407407407407407, 0.07142857142857142, 0.08333333333333333\n",
    "    ])\n",
    "    \n",
    "    check_equal(df_jaccard.tail(10)[\"jaccard_prediction\"].tolist(), [0, 0, 0, 4, 4, 1, 1, 1, 1, 1])\n",
    "    \n",
    "    jaccard_accuracy = (df_jaccard[\"jaccard_prediction\"] == df_jaccard[\"answer_sent_index\"]).values.mean()\n",
    "    check_approx(jaccard_accuracy, 0.7001992605475634)\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_get_jaccard_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the Jaccard technique is saying \"the context sentence that is most similar to the question contains the answer.\" We see that even such a simple heuristic can achieve 70% accuracy, which is not bad at all. This performance can be improved a bit if we take the time to preprocess (e.g., remove stopwords, lemmatize tokens) in the questions and context sentences, before computing the Jaccard overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: TF-IDF Vectors\n",
    "Instead of Jaccard overlap, we can employ other measures of similarity, such as the Euclidean distance:\n",
    "\n",
    "$$d_{\\text{euclidean}}(a, b) = \\|a - b\\|_2 = \\sqrt{\\sum_{i=1}^k (a_i - b_i)^2}.$$\n",
    "\n",
    "To compute this distance, we first need to convert each question and context sentence into a vector. Recall from earlier projects that one way to do so is building a TF-IDF model, which transforms each string into a vector $v \\in \\mathbb{R}^k$ where:\n",
    "* $k$ is the number of unique tokens in the entire dataset.\n",
    "* The $i$-th element is the frequency of token $i$ in the string, divided by its IDF value.\n",
    "\n",
    "Given a question, a list of context sentences, and a trained TF-IDF model, we can identify the answer sentence as follows:\n",
    "1. Transform the question into a vector $u$ using the TF-IDF model.\n",
    "1. Transform each context sentence $s_i$ into a vector $v_i$ using the TF-IDF model.\n",
    "1. Return the index of the context sentence whose Euclidean distance to the input question is smallest in the TF-IDF space:\n",
    "\n",
    "$$\\hat y = \\underset{1 \\le i \\le m}{\\operatorname{argmin}} d_{\\text{euclidean}}(u, v_i).$$\n",
    "\n",
    "Implement the function `get_tfidf_prediction` that performs the above steps on the dataset `df_squad`. For every row, it stores the predicted answer sentence index in a new column `\"tfidf_prediction\"`, and the corresponding smallest Euclidean distance value in a new column `\"distance_value\"`.\n",
    "\n",
    "**Notes**:\n",
    "* Since the input `tfidf_vectorizer` is already trained, you don't need to fit it on anything. Instead, just call `.transform` on the appropriate question / context sentence.\n",
    "* Because TF-IDF transformation outputs sparse matrices, you should only use `scipy.sparse` methods to operate on them. Using standard NumPy/Scipy methods may lead to dimension issues or implicit conversion of the sparse matrices to dense.\n",
    "* If multiple context sentences have the same (smallest) distance value from the question, return the smallest sentence index.\n",
    "* You may find that using `df.apply(<your_custom_function>, axis = 1)` to process every row is quite slow, due to the complexity of the operations involved. To work around this issue, try to think about how to completely vectorize this function, using only built-in Pandas and NumPy/Scipy or Sklearn operations.\n",
    "* You may find the Pandas method `.explode()` helpful. Note that there are duplicate questions in the dataset (the same question may apply to different context paragraphs), so be careful when performing groupby on the exploded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def get_tfidf_prediction(df_squad, tfidf_vectorizer):\n",
    "    \"\"\"\n",
    "    Identify the answer sentence as one whose TF-IDF representation has minimal distance to that of the question.\n",
    "    \n",
    "    args:\n",
    "        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n",
    "        tfidf_vectorizer (sklearn.feature_extraction.text.TfidfVectorizer) :\n",
    "            the TF-IDF model to transform questions and sentences\n",
    "        \n",
    "    returns:\n",
    "        pd.DataFrame : the input dataframe with two additional columns, \"tfidf_prediction\" and \"distance_value\"\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for index, row in df_squad.iterrows():\n",
    "        question_vector = tfidf_vectorizer.transform([row['question']])\n",
    "        sentence_vectors = tfidf_vectorizer.transform(row['context_sentences'])\n",
    "        distances = euclidean_distances(question_vector, sentence_vectors).flatten()\n",
    "        min_index = np.argmin(distances)\n",
    "        min_distance = distances[min_index]\n",
    "        results.append([min_index, min_distance])\n",
    "\n",
    "    result_df = pd.DataFrame(results, columns=['tfidf_prediction', 'distance_value'])\n",
    "    df_squad[['tfidf_prediction', 'distance_value']] = result_df\n",
    "\n",
    "    return df_squad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example trained TF-IDF vectorizer that we can use. Since fitting it on the entire dataset takes a while, we will initialiize and fit the vectorizer in the global namespace, so that it can be reused in all subsequent functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
       "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
       "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n",
       "                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                            &#x27;itself&#x27;, ...],\n",
       "                tokenizer=&lt;function word_tokenize at 0x7f2b818e4790&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;, &#x27;our&#x27;, &#x27;ours&#x27;,\n",
       "                            &#x27;ourselves&#x27;, &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;, &quot;you&#x27;ll&quot;,\n",
       "                            &quot;you&#x27;d&quot;, &#x27;your&#x27;, &#x27;yours&#x27;, &#x27;yourself&#x27;, &#x27;yourselves&#x27;,\n",
       "                            &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;,\n",
       "                            &#x27;her&#x27;, &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                            &#x27;itself&#x27;, ...],\n",
       "                tokenizer=&lt;function word_tokenize at 0x7f2b818e4790&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(min_df=10, ngram_range=(1, 2),\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                tokenizer=<function word_tokenize at 0x7f2b818e4790>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer = nltk.word_tokenize,\n",
    "    stop_words = stopwords.words('english'),\n",
    "    ngram_range = (1,2),\n",
    "    max_df = 1.0,\n",
    "    min_df = 10\n",
    ")\n",
    "tfidf_vectorizer.fit(df_squad[\"context_paragraph\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "CPU times: user 4min 1s, sys: 31.3 ms, total: 4min 1s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "def test_get_tfidf_prediction():\n",
    "    \"\"\"Test on the first 10 rows\"\"\"\n",
    "    df_tf_idf = get_tfidf_prediction(df_squad.head(10).copy(), tfidf_vectorizer)\n",
    "    \n",
    "    check_equal(df_tf_idf.shape, (10, 9))\n",
    "    \n",
    "    check_approx(df_tf_idf[\"distance_value\"].tolist(), [\n",
    "        1.4142135623730951, 1.4142135623730951, 1.118805210307003, 1.414213562373095,\n",
    "        1.414213562373095, 1.0991965705062294, 1.2823059131390453, 1.1299491754496973,\n",
    "        1.3217983153692023, 1.1364153055563095\n",
    "    ])\n",
    "    \n",
    "    \"\"\"Test on the whole dataset\"\"\"\n",
    "    df_tf_idf = get_tfidf_prediction(df_squad.copy(), tfidf_vectorizer)\n",
    "    \n",
    "    check_equal(df_tf_idf.shape, (86821, 9))\n",
    "    \n",
    "    check_approx(df_tf_idf.tail(10)[\"distance_value\"].tolist(), [\n",
    "        1.2205482532513834, 1.1011579092263701, 1.23935316383152, 1.2848863907388077,\n",
    "        1.1456258185112482, 1.2657059224645815, 1.4142135623730951, 1.2768968376533885,\n",
    "        1.2666294346869091, 1.4142135623730951\n",
    "    ])\n",
    "    \n",
    "    tfidf_accuracy = (df_tf_idf[\"tfidf_prediction\"] == df_tf_idf[\"answer_sent_index\"]).values.mean()\n",
    "    assert 0.68 <= round(tfidf_accuracy, 2) <= 0.69, tfidf_accuracy\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "%time test_get_tfidf_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is sufficiently optimized, you should expect to see the local test being finished in about 3 minutes or less, on a `STANDARD_NC8AS_T4_V3` GPU Compute. If your code runs for more than 7 minutes, you should try to improve its efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still see a training accuracy of about 0.68, so TF-IDF is fairly similar in performance to the baseline Jaccard model.\n",
    "\n",
    "Up until now we have used language models that rely only on word frequencies, without considering the meaning of the words themselves. Now we will address this shortcoming by considering the *word embedding* of each word in our corpus.  Roughly speaking, a word embedding is a vector representation of that word in some space $\\mathbb{R}^k$. This representation differs from the TF-IDF transformation in two important ways:\n",
    "\n",
    "1. If two words have similar meanings in some sense, their Euclidean distances should be close. For example, we may expect the word `\"Pittsburgh\"` to be closer, in Euclidean distance, to `\"Chicago\"` than to `\"Pikachu\"`, because the first two are city names while the third is a Pokemon.\n",
    "1. The dimensionality of the vector $k$ is fixed and typically much smaller than the vocabulary size.\n",
    "\n",
    "While these features sound promising, constructing word embeddings requires a very large amount of data (you need to see `\"Pittsburgh\"` and `\"Chicago\"` appear together in overlapping context enough times for the model to learn that they are similar). The algorithms to train word embedding models are unfortunately beyond the scope of this course, as they involve many machine learning theories we haven't covered.\n",
    "\n",
    "That said, there are many powerful pre-trained models that we can use. These models have been trained on huge amounts of data and encode a lot of information about the meanings of the words. The first pre-trained model we will use is one from the [SentenceTransformer library](https://github.com/UKPLab/sentence-transformers#getting-started) called `'distilbert-base-nli-stsb-mean-tokens'`. Once loaded, it can encode a collection of strings, yielding a matrix where each row is the vector embedding of one string.\n",
    "\n",
    "Run the following cell to see the model in action. Note that the first time you do so, it may take some time to download the model. Also note that the embedding matrix is a dense NumPy matrix, rather than a sparse Scipy matrix like what TF-IDF outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 768)\n",
      "[[-0.21486165  0.395723    0.469087   ... -0.23119043 -0.4957917\n",
      "   0.42366374]\n",
      " [-0.4400171  -0.28488502  0.23363815 ...  0.11956113 -0.16530254\n",
      "  -0.0862514 ]\n",
      " [-0.29504815 -0.24928899 -0.02407092 ...  0.1194457   0.00626569\n",
      "   1.0400687 ]]\n"
     ]
    }
   ],
   "source": [
    "sent_transformer = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "embedding = sent_transformer.encode([\n",
    "    'This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown fox jumps over the lazy dog.'\n",
    "])\n",
    "print(embedding.shape)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Sent2vec Encoders\n",
    "Here we will follow roughly the same procedure as in the previous question: first convert the questions and context sentences into vectors using `sent_transformer`, then identify the context sentence whose vector representation is closest in Euclidean distance to that of the input question. However, one important caveat is that encoding the questions and context sentences with `sent_transformer` takes a lot longer (as much as 10 times longer) than with `tfidf_vectorizer`, because this encoding is actually the forward pass through a pre-trained neural network. To address this issue, we recommend the following outline for your implementation:\n",
    "\n",
    "1. Construct a mapping from each unique question / context sentence to its vector encoding.\n",
    "1. Use this mapping to compute the vector representation of every question / context sentence in the dataset.\n",
    "1. Compute the Euclidean distances between the questions and context sentences to identify the answer sentence index for each question.\n",
    "\n",
    "The key idea is that there are some duplicate questions and many duplicate context sentences (since several questions share the same context paragraph), so you want to store the encoding of each unique question / context sentence to avoid recomputing them several times. This same idea also applies to Question 2, although repeated computations are not as big of an issue there since TF-IDF transformations are fast.\n",
    "\n",
    "Implement the function `get_sent2vec_prediction` that performs the above steps on the dataset `df_squad`. For every row, it stores the predicted answer sentence index in a new column `\"sent2vec_prediction\"`, and the corresponding smallest Euclidean distance value in a new column `\"distance_value\"`.\n",
    "\n",
    "**Notes**:\n",
    "* When working with only the values of a Series (e.g., a dataframe column) and you do not care about its name or indices, calling the `.values` attribute to convert it to a NumPy array may provide some speed-up.\n",
    "* If multiple context sentences have the same (smallest) distance value from the question, return the smallest sentence index.\n",
    "* You should encode the entire set of unique questions / context sentences at once, rather then encoding each of them individually in a loop. Due to the encoder's internal implementation, you will get different embedding results if you encode each question / context sentence individually.\n",
    "* Be careful when dealing with nested NumPy arrays. If you get a NumPy array that contains other NumPy arrays, you can use `np.stack` to turn it to a normal multi-dimensional array (otherwise it will be treated as a 1D array of pointers to other arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent2vec_prediction(df_squad, encoder):\n",
    "    \"\"\"\n",
    "    Identify the answer sentence as one whose Sent2vec representation has minimal distance to that of the question.\n",
    "    \n",
    "    args:\n",
    "        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n",
    "        encoder (SentenceTransformer) :\n",
    "            the Sent2vec encoder used to transform questions and sentences into their word embeddings\n",
    "        \n",
    "    returns: Tuple(question_embeddings, context_embeddings, df_sent2vec)\n",
    "        question_embeddings (Dict[str, np.ndarray]) :\n",
    "            a mapping between each unique question and its Sent2vec embedding\n",
    "        context_embeddings (Dict[str, np.ndarray]) :\n",
    "            a mapping between each unique context sentence and its Sent2vec embedding\n",
    "        df_sent2vec (pd.DataFrame) :\n",
    "            the input dataframe with two additional columns, \"sent2vec_prediction\" and \"distance_value\"\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    unique_questions = df_squad['question'].unique()\n",
    "    unique_sentences = pd.Series(df_squad['context_sentences'].explode().unique())\n",
    "    question_embeddings = dict(zip(unique_questions, encoder.encode(unique_questions, convert_to_tensor=False)))\n",
    "    context_embeddings = dict(zip(unique_sentences, encoder.encode(unique_sentences.to_list(), convert_to_tensor=False)))\n",
    "    \n",
    "    for index, row in df_squad.iterrows():\n",
    "        question_vec = question_embeddings[row['question']]\n",
    "        sentence_vecs = np.stack([context_embeddings[sent] for sent in row['context_sentences']])\n",
    "        distances = euclidean_distances([question_vec], sentence_vecs).flatten()\n",
    "        min_index = np.argmin(distances)\n",
    "        min_distance = distances[min_index]\n",
    "        results.append([min_index, min_distance])\n",
    "        \n",
    "    result_df = pd.DataFrame(results, columns=['sent2vec_prediction', 'distance_value'])\n",
    "    df_squad[['sent2vec_prediction', 'distance_value']] = result_df\n",
    "\n",
    "    return (question_embeddings, context_embeddings, df_squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "Saving the embedding to pickle files for later use ...\n",
      "Done!\n",
      "CPU times: user 3min 34s, sys: 49.7 s, total: 4min 24s\n",
      "Wall time: 3min 18s\n"
     ]
    }
   ],
   "source": [
    "def test_get_sent2vec_prediction():\n",
    "    \"\"\"Test on the first 10 rows\"\"\"\n",
    "    question_embeddings_map, context_embeddings_map, df_sent2vec = \\\n",
    "        get_sent2vec_prediction(df_squad.head(10).copy(), sent_transformer)\n",
    "    \n",
    "    question = 'When did Beyoncé rise to fame?'\n",
    "    check_approx(\n",
    "        question_embeddings_map[question][:10],\n",
    "            [-0.32787397503852844, -0.15557105839252472, 0.6588357090950012, -0.6630659699440002,\n",
    "             0.5884889960289001, -0.04990821331739426, 0.4931581914424896, 0.24734026193618774,\n",
    "             -0.278196781873703, 0.6030771732330322]\n",
    "    )\n",
    "    \n",
    "    question = 'In what city and state did Beyonce  grow up? '\n",
    "    check_approx(\n",
    "        question_embeddings_map[question][:10],\n",
    "            [-0.010077972896397114, -0.3763282299041748, 0.939608097076416, -0.580588161945343,\n",
    "             0.10827723145484924, 0.25717461109161377, 0.6128496527671814, 0.5031235814094543,\n",
    "             -0.4418582022190094, 0.4174884855747223]\n",
    "    )\n",
    "    \n",
    "    context = \"Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child.\"\n",
    "    check_approx(\n",
    "        context_embeddings_map[context][:10],\n",
    "            [-0.10903950035572052, -0.27940085530281067, 0.23572109639644623, 0.5180771946907043,\n",
    "             0.3553291857242584, 0.13151134550571442, 0.17776617407798767, -0.4766906797885895,\n",
    "             -0.09587486833333969, 0.9343098998069763]\n",
    "    )\n",
    "    \n",
    "    context = \"Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time.\"\n",
    "    check_approx(\n",
    "        context_embeddings_map[context][:10],\n",
    "            [-0.8026146292686462, 0.13804392516613007, 0.5916290283203125, 0.39244064688682556,\n",
    "             -0.2768344283103943, 0.46939727663993835, 0.41246476769447327, 0.300344318151474,\n",
    "             -1.0914496183395386, -0.1786373406648636]\n",
    "    )\n",
    "\n",
    "    check_approx(\n",
    "        df_sent2vec[\"distance_value\"],\n",
    "        [12.849681854248047, 13.180705070495605, 11.950621604919434, 13.039735794067383, 12.867680549621582,\n",
    "         13.494372367858887, 15.975934028625488, 17.264524459838867, 12.373725891113281, 12.654834747314453]\n",
    "    )\n",
    "    \n",
    "    \"\"\"Test on the full dataset\"\"\"\n",
    "    question_embeddings_map, context_embeddings_map, df_sent2vec = \\\n",
    "        get_sent2vec_prediction(df_squad.copy(), sent_transformer)\n",
    "    \n",
    "    question = 'What is KMC an initialism of?'\n",
    "    check_approx(\n",
    "        question_embeddings_map[question][:10],\n",
    "        [-0.9376349 ,  0.07794607, -0.08829201,  0.23485802,  0.05154163,\n",
    "         -0.11316115, -0.15181658,  0.69910896,  0.43863776, -0.5214241]\n",
    "    )\n",
    "    \n",
    "    question = 'In what year did Kathmandu create its initial international relationship?'\n",
    "    check_approx(\n",
    "        question_embeddings_map[question][:10],\n",
    "        [0.20806803,  0.5848249 ,  0.51129144, -0.8319231 ,  0.10700534,\n",
    "        0.35735554,  0.4291537 ,  1.0774763 , -0.12320331,  0.5771949 ]\n",
    "    )\n",
    "    \n",
    "    context = \"KMC's first international relationship was established in 1975 with the city of Eugene, Oregon, United States.\"\n",
    "    check_approx(\n",
    "        context_embeddings_map[context][:10],\n",
    "        [0.65290284,  0.1876768 ,  0.5210961 , -0.13445881, -0.05164678,\n",
    "        0.5333182 ,  0.6115602 ,  0.6255963 ,  0.38067245, -0.10421762]\n",
    "    )\n",
    "    \n",
    "    context = 'It was established in 1972 and started to impart medical education from 1978.'\n",
    "    check_approx(\n",
    "        context_embeddings_map[context][:10],\n",
    "        [0.72947043, -0.5591796 ,  0.86937994, -0.80692345, -0.05610372,\n",
    "        0.07088739,  1.1165347 ,  0.8680078 ,  0.04112893, -0.71701765]\n",
    "    )\n",
    "    check_approx(\n",
    "        df_sent2vec[\"distance_value\"].tail(10),\n",
    "        [11.888211250305176, 13.456267356872559, 14.015336990356445, 14.333199501037598,\n",
    "         11.331104278564453, 10.903414726257324, 17.290882110595703, 14.260327339172363,\n",
    "         13.246850967407227, 16.56328582763672]\n",
    "    )\n",
    "    \n",
    "    sent2vec_accuracy = (df_sent2vec[\"sent2vec_prediction\"] == df_sent2vec[\"answer_sent_index\"]).mean()\n",
    "    check_equal(round(sent2vec_accuracy, 2), 0.68)\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "    print(\"Saving the embedding to pickle files for later use ...\")\n",
    "    with open(\"question_embeddings_map.pkl\", \"wb\") as f1, open(\"context_embeddings_map.pkl\", \"wb\") as f2:\n",
    "        pickle.dump(question_embeddings_map, f1)\n",
    "        pickle.dump(context_embeddings_map, f2)\n",
    "    print(\"Done!\")\n",
    "        \n",
    "    \n",
    "%time test_get_sent2vec_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is sufficiently optimized, you should expect to see the local test finished in about 7.5 minutes or less, on a `STANDARD_NC8AS_T4_V3` GPU Compute. If your code runs for more than 10 minutes, you should try to improve its efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a slight improvement in accuracy, compared to the previous two methods. It seems like using the meanings of the words isn't too effective here. Now we will try a different technique that utilizes the linguistic structures of the questions and context sentences. Let's walk through an example of what we will do first.\n",
    "\n",
    "Assume we have the following input `question`:\n",
    "\n",
    "```\n",
    "How many parameters does BERT-large have?\n",
    "```\n",
    "and `context`:\n",
    "```\n",
    "BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\n",
    "```\n",
    "\n",
    "We will perform the following steps:\n",
    "1. Lowercase `question` and identify its *root word*:\n",
    "```py\n",
    "question_root = \"have\"\n",
    "```\n",
    "1. Lemmatize this root word to reduce it to its base form.\n",
    "```py\n",
    "lemmatized_question_root = \"have\"\n",
    "```\n",
    "1. Split `context` into context sentences and lowercase them:\n",
    "```py\n",
    "sent1 = \"bert-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340m parameters!\"\n",
    "sent2 = \"altogether it is 1.34gb, so expect it to take a couple minutes to download to your colab instance.\"\n",
    "```\n",
    "1. Identify the *noun chunks* in each context sentence:\n",
    "```py\n",
    "sent1_ncs = [\"it\", \"24-layers\", \"an embedding size\", \"a total\", \"340m\", \"parameters\"]\n",
    "sent2_ncs = [\"it\", \"1.34gb\", \"it\", \"a couple minutes\", \"your colab instance\"]\n",
    "```\n",
    "1. Extract the *root word* from each noun chunk:\n",
    "```py\n",
    "sent1_nc_roots = [\"it\", \"layers\", \"size\", \"total\", \"parameters\"]\n",
    "sent2_nc_roots = [\"it\", \"gb\", \"it\", \"minutes\", \"instance\"]\n",
    "```\n",
    "1. Identify and lemmatize the *head* for each of the above root words. Store these heads into sets:\n",
    "```py\n",
    "sent1_nc_root_heads = {\"have\", \"of\", \"layer\", \"for\"}\n",
    "sent2_nc_root_heads = {\"to\", \"take\", \"is\"}\n",
    "```\n",
    "1. Return the index of the first context sentence whose `nc_root_heads` set contains the question's root word `lemmatized_question_root`. If no context sentence meets this requirement, return 0 (in other words, we predict that the first context sentence is the answer, based on the assumption that the first sentence in a paragraph typically contains the most important information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define some global variables that will be employed in this task. In particular, we will use the `en_core_web_sm` model from spaCy, and the part-of-speech lemmatization procedure from Project 4. Note: the autograder will use this cell so do not change its content and do not add the tag `excluded_from_script`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pos_mapping = {\n",
    "    'ADJ' : wordnet.ADJ, 'NOUN' : wordnet.NOUN,\n",
    "    'VERB' : wordnet.VERB, 'ADP' : wordnet.ADV\n",
    "}\n",
    "\n",
    "def lemmatize_token(token):\n",
    "    \"\"\"\n",
    "    Lemmatize a spaCy token based on its part-of-speech tag. If a tag is not recognized, treat it as a noun.\n",
    "    \n",
    "    args:\n",
    "        token (spacy.tokens.token.Token) : an output token when inputting a raw string to a spaCy model\n",
    "    \n",
    "    return:\n",
    "        str : the lemmatized string\n",
    "    \"\"\"\n",
    "    return lemmatizer.lemmatize(token.text, pos = pos_mapping.get(token.pos_, wordnet.NOUN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we recommend consulting the [AST primer](https://nbviewer.jupyter.org/url/clouddatascience.blob.core.windows.net/primers/ast-primer/ast_primer.ipynb) and the [spaCy documentation](https://spacy.io/usage/linguistic-features) to replicate the above example in code. Once you have successfully done so, proceed to the next question.\n",
    "\n",
    "**Notes**:\n",
    "* To identify a question's root word, you can input it to the `en_nlp` model, extract the first sentence from the `.sents` generator, and use the `.root` attribute. This returns a `Token` that you can then input to `lemmatize_token` to get the lemmatized form.\n",
    "* Remember to also lemmatize the heads of the root words for the noun chunks (in the above example, `sent1_nc_root_heads` and `sent2_nc_root_heads` contain lemmatized strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "question = \"How many parameters does BERT-large have?\"\n",
    "context_sentences = [\n",
    "    \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters!\",\n",
    "    \"Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n",
    "]\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Root word matching\n",
    "Implement the function `get_rootword_prediction` that performs the above steps on the dataset `df_squad`. For every row, it stores:\n",
    "* the predicted answer sentence index in a new column `\"rootword_prediction\"`\n",
    "* the lemmatized root word of the question in a new column `\"question_root\"`\n",
    "* the set of lemmatized heads of the root words for the noun chunks in the predicted context sentence (for the above example, this would be the set `sent1_nc_root_heads`) in a new column `\"nc_root_heads\"`.\n",
    "\n",
    "**Notes**:\n",
    "* Recall that the context paragraphs have already been split into sentences in the column `context_sentences`.\n",
    "* Remember to lower case all the questions and context sentences when doing root word matching (but do not change the original `question`, `context_paragraph` or `context_sentences` columns in `df_squad`).\n",
    "* If a question contains multiple sentences, make sure you extract the question root word from the **first sentence**, not the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rootword_prediction(df_squad, en_nlp):\n",
    "    \"\"\"\n",
    "    Identify the answer sentence as the first sentence whose list of heads of the root words for its noun chunks\n",
    "    contains the question's root word.\n",
    "    \n",
    "    args:\n",
    "        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n",
    "        en_nlp (spacy.lang.en.English) : a pre-trained SpaCy language model\n",
    "    \n",
    "    returns:\n",
    "        pd.DataFrame : the input dataframe with three additional columns,\n",
    "            \"rootword_prediction\", \"question_root\" and \"nc_root_heads\"\n",
    "    \"\"\"\n",
    "    def analyze_text(row):\n",
    "        question_doc = en_nlp(row['question'].lower())\n",
    "        main_verb_token = next(question_doc.sents).root\n",
    "        main_verb = lemmatize_token(main_verb_token)\n",
    "        best_match_index = 0\n",
    "        best_match_heads = set()\n",
    "\n",
    "        for index, sentence in enumerate(row['context_sentences']):\n",
    "            context_doc = en_nlp(sentence.lower())\n",
    "            heads_of_noun_chunks = set()\n",
    "            \n",
    "            for nc in context_doc.noun_chunks:\n",
    "                head_of_root = lemmatize_token(nc.root.head)\n",
    "                heads_of_noun_chunks.add(head_of_root)\n",
    "\n",
    "            if main_verb in heads_of_noun_chunks:\n",
    "                best_match_index = index\n",
    "                best_match_heads = heads_of_noun_chunks\n",
    "                break\n",
    "\n",
    "        if not best_match_heads:\n",
    "            first_sentence_doc = en_nlp(row['context_sentences'][0].lower())\n",
    "            \n",
    "            for nc in first_sentence_doc.noun_chunks:\n",
    "                head_of_root = lemmatize_token(nc.root.head)\n",
    "                best_match_heads.add(head_of_root)\n",
    "\n",
    "        return pd.Series([best_match_index, main_verb, best_match_heads],\n",
    "                         index=['rootword_prediction', 'question_root', 'nc_root_heads'])\n",
    "\n",
    "    df_squad[['rootword_prediction', 'question_root', 'nc_root_heads']] = df_squad.apply(analyze_text, axis=1)\n",
    "\n",
    "    return df_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n",
      "CPU times: user 2min 44s, sys: 75.8 ms, total: 2min 44s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "def test_rootword_prediction():\n",
    "    \"\"\"Test on the first 10 rows\"\"\"\n",
    "    df_rootword = get_rootword_prediction(df_squad.head(10).copy(), en_nlp)\n",
    "    check_equal(df_rootword.shape, (10, 10))\n",
    "    \n",
    "    check_equal(df_rootword[\"rootword_prediction\"].tolist(), [0, 0, 0, 1, 2, 1, 0, 0, 0, 0])\n",
    "    \n",
    "    check_equal(df_rootword[\"nc_root_heads\"].tolist(), [\n",
    "        {'carter', 'producer', 'singer', 'songwriter', 'is'},\n",
    "        {'carter', 'producer', 'singer', 'songwriter', 'is'},\n",
    "        {'carter', 'producer', 'singer', 'songwriter', 'is'},\n",
    "        {'in', 'perform', 'of', 'to', 'houston', 'as'},\n",
    "        {'become', 'father', 'of', 'by'},\n",
    "        {'in', 'perform', 'of', 'to', 'houston', 'as'},\n",
    "        {'carter', 'producer', 'singer', 'songwriter', 'is'},\n",
    "        {'carter', 'producer', 'singer', 'songwriter', 'is'},\n",
    "        {'carter', 'producer', 'singer', 'songwriter', 'is'},\n",
    "        {'carter', 'producer', 'singer', 'songwriter', 'is'}\n",
    "    ])\n",
    "    \n",
    "    check_equal(df_rootword[\"question_root\"].tolist(), [\n",
    "        'start', 'compete', 'leave', 'in', 'become', 'in', 'make', 'manage', 'rise', 'have'\n",
    "    ])\n",
    "    \n",
    "    \"\"\"Test on 5000 sampled data points\"\"\"\n",
    "    df_rootword = get_rootword_prediction(df_squad.sample(5000, random_state = 0).copy(), en_nlp)\n",
    "    check_equal(df_rootword.shape, (5000, 10))\n",
    "    \n",
    "    check_equal(df_rootword[\"rootword_prediction\"].value_counts().to_dict(), \n",
    "        {0: 3547, 1: 523, 2: 394, 3: 244, 4: 156, 5: 64, 6: 41, 7: 18, 8: 4, 9: 4, 10: 2, 11: 2, 21: 1}\n",
    "    )\n",
    "    \n",
    "    check_equal(df_rootword[\"rootword_prediction\"].tail(10).tolist(), [0, 1, 3, 1, 0, 0, 1, 2, 0, 4])\n",
    "    \n",
    "    check_equal(df_rootword[\"question_root\"].tail(10).tolist(), [\n",
    "        'see', 'kill', 'in', 'vote', 'wa', 'is', 'say', 'wa', 'compile', 'assign'\n",
    "    ])\n",
    "    \n",
    "    check_equal(df_rootword[\"nc_root_heads\"].tail(10).tolist(), [\n",
    "        {'announce', 'by', 'from', 'heed', 'in', 'live'},\n",
    "        {'destroy', 'in', 'kill', 'of'},\n",
    "        {'be', 'in', 'park'},\n",
    "        {'arouse', 'fuel', 'in', 'of', 'studdard', 'than', 'vote'},\n",
    "        {'at', 'by', 'hold', 'in', 'louis', 'of', 'on'},\n",
    "        {'achieve', 'as', 'garde', 'have', 'in', 'on', 'skalkottas', 'with', 'xenakis'},\n",
    "        {'call', 'is', 'of', 'say', 'statement'},\n",
    "        {'after', 'by', 'find', 'of', 'say', 'wa'},\n",
    "        {'architect', 'artist', 'attract', 'from', 'in', 'of', 'ruler', 'through'},\n",
    "        {'assign', 'at', 'by', 'glanville', 'like', 'manage', 'of', 'teach', 'wa', 'with'}\n",
    "    ])\n",
    "    \n",
    "    accuracy = (df_rootword[\"rootword_prediction\"] == df_rootword[\"answer_sent_index\"]).values.mean()\n",
    "    check_equal(accuracy, 0.4626)\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "%time test_rootword_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this procedure takes a very long time, we only tested it on a random sample of 5000 data points. If your code is sufficiently optimized, the local test should finish in about 4 minutes on a `STANDARD_NC8AS_T4_V3` compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Supervised Models\n",
    "So far we have been exploring unsupervised methods for answer extraction which involves dividing the questions and contexts into tokens and projecting those tokens into a common representation space. You may notice that their performances weren't particularly great because we didn't perform any training on the dataset; instead, we only used pre-defined heuristics and pre-trained models. From this point, we will move to the supervised learning domain, where we make use of the ground-truth answers and build models that learn from these answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Preparing dataset for supervised learning\n",
    "We will first consider a binary classification setting, where we are given a question and a context sentence, and need to predict whether this context sentence contains the answer. In this setting, we can get several training data points from each row in the original dataset `df_squad`. In particular, if a row in `df_squad` looks like the following:\n",
    "\n",
    "|question|context_sentences|answer_sent_index|\n",
    "|---|---|---|\n",
    "|`q`|`[s0, s1, s2, s3]`|2|\n",
    "\n",
    "then it contributes four data points:\n",
    "\n",
    "|question|context_sentence|is_answer_sent|\n",
    "|---|---|---|\n",
    "|`q`|`s0`|0|\n",
    "|`q`|`s1`|0|\n",
    "|`q`|`s2`|1|\n",
    "|`q`|`s3`|0|\n",
    "\n",
    "More generally, a row in `df_squad` where the `context_sentences` list has `n` sentences will be transformed into `n` rows, one for each context sentence. Among these new rows, only the row at index `answer_sent_index` gets assigned the label 1, while the others get the label 0.\n",
    "\n",
    "Implement the function `build_data_for_classification` that turns the original dataset `df_squad` into a dataframe with 3 columns -- `question`, `context_sentence` and `is_answer_sent` -- using the procedure specified above.\n",
    "\n",
    "**Notes**:\n",
    "* Keep in mind that the new dataframe has a column named `context_sentence`, **not** `context_sentences`.\n",
    "* You should preserve the original row ordering in the input dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_for_classification(df_squad):\n",
    "    \"\"\"\n",
    "    Convert the SQuAD dataset into a format where every row contains one question, one context answer,\n",
    "    and a flag that indicates whether the context sentence is the answer.\n",
    "    \n",
    "    args:\n",
    "        df_squad (pd.DataFrame) : a copy of the SQuAD dataset\n",
    "        \n",
    "    returns:\n",
    "        pd.DataFrame : a new dataframe with 3 columns: question, context_sentence, is_answer_sent\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for _, entry in df_squad.iterrows():\n",
    "        query = entry['question']\n",
    "        correct_answer_index = entry['answer_sent_index']\n",
    "        all_context_sentences = entry['context_sentences']\n",
    "\n",
    "        for index, context_sentence in enumerate(all_context_sentences):\n",
    "            answer_flag = 1 if index == correct_answer_index else 0\n",
    "            data.append({\n",
    "                'question': query, \n",
    "                'context_sentence': context_sentence, \n",
    "                'is_answer_sent': answer_flag\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_build_data_for_classification():\n",
    "    \"\"\"Test on 10 random rows\"\"\"\n",
    "    df_sample = df_squad.sample(n = 10, random_state = 0).copy()\n",
    "    df_formatted = build_data_for_classification(df_sample)\n",
    "    \n",
    "    assert df_formatted.shape == (48, 3), df_formatted.shape\n",
    "    \n",
    "    assert sorted(df_formatted.columns) == ['context_sentence', 'is_answer_sent', 'question']\n",
    "    \n",
    "    assert df_formatted['is_answer_sent'].tolist() == [\n",
    "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
    "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "        1, 0, 0, 0, 1, 0, 1, 0\n",
    "    ], df_formatted['is_answer_sent'].tolist()\n",
    "    \n",
    "    # check that question orderings are preserved\n",
    "    assert (df_formatted[\"question\"].unique() == df_sample[\"question\"].unique()).all()\n",
    "    \n",
    "    \"\"\"Test on the full dataset\"\"\"\n",
    "    df_formatted = build_data_for_classification(df_squad.copy())\n",
    "    \n",
    "    assert df_formatted.shape == (443259, 3), df_formatted.shape\n",
    "    \n",
    "    assert df_formatted['is_answer_sent'].sample(n = 40, random_state = 200).tolist() == [\n",
    "        1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
    "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 1, 0, 0, 0, 0, 1, 0, 1\n",
    "    ], df_formatted['is_answer_sent'].tail(40).tolist()\n",
    "    \n",
    "    assert (df_formatted[\"question\"].unique() == df_squad[\"question\"].unique()).all()\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_build_data_for_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building our supervised learning models, we will load the embeddings you created in Question 3 and set up the train set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "question_embeddings_map = pd.read_pickle(\"question_embeddings_map.pkl\")\n",
    "context_embeddings_map = pd.read_pickle(\"context_embeddings_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "df_squad_train, df_squad_test = train_test_split(df_squad, train_size = 0.8, random_state = 0)\n",
    "df_train_formatted = build_data_for_classification(df_squad_train.reset_index())\n",
    "df_test_formatted = build_data_for_classification(df_squad_test.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Logistic Regression\n",
    "Having set up the dataset for binary classification, we can now train a logistic regression model. While the binary labels are already set up, we still need to construct the feature vectors as follows. For every question `q` and context sentence `s`:\n",
    "* Convert the question to its word embedding $x_q \\in \\mathbb{R}^k$, using the question embedding map from Question 3.\n",
    "* Convert the context sentence to its word embedding $x_s \\in \\mathbb{R}^l$, using the context embedding map from Question 3.\n",
    "* Concatenate these two vectors to get the input vector to the logistic regression model:\n",
    "\n",
    "$$x_{q,s} = (x_{q1} \\quad x_{q2} \\quad \\ldots \\quad x_{qk} \\quad x_{s1} \\quad x_{s2} \\quad \\ldots \\quad x_{sl})^\\top \\in \\mathbb{R}^{k+l}.$$\n",
    "\n",
    "Implement the function `get_lr_prediction` that performs the following steps:\n",
    "\n",
    "1. Construct the feature vector for each row of the train set `df_train_formatted`, using the above formula.\n",
    "1. Use an Sklearn `StandardScaler` (with default parameters) to fit and transform the train set `df_train_formatted`.\n",
    "1. Train an Sklearn `LogisticRegression` model on the train set `df_train_formatted`.\n",
    "1. Use this model to perform prediction on the test set `df_test_formatted`.\n",
    "1. Return the trained LR model and its accuracy on the test set (i.e., the number of correct predictions divided by the test set size).\n",
    "\n",
    "**Notes**:\n",
    "* When creating a `LogisticRegression` model you should set `random_state` to the input `seed` and `max_iters` to 1000. You do not need to specify any other parameter.\n",
    "* Make sure you also standardize the feature matrix built from the test set before inputting it to the LR model for prediction.\n",
    "* Similar to Question 3, if you get a NumPy array that contains other NumPy arrays, you can use `np.stack` to turn it to a normal multi-dimensional array (otherwise it will be treated as a 1D array of pointers to other arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_lr_prediction(df_train_formatted, df_test_formatted, question_embeddings_map, context_embeddings_map, seed = 0):\n",
    "    \"\"\"\n",
    "    Train and evaluate the performance of a binary logisitic regression model to predict\n",
    "    whether a context sentence contains the answer to a given question.\n",
    "    \n",
    "    args:\n",
    "        df_train_formatted (pd.DataFrame) : the train set dataframe with 3 columns:\n",
    "            question, context_sentence, is_answer_sent\n",
    "        df_test_formatted (pd.DataFrame) : the test set dataframe with 3 columns:\n",
    "            question, context_sentence, is_answer_sent\n",
    "        question_embeddings_map (dict[str, np.ndarray]) : a mapping from question to word embedding\n",
    "        context_embeddings_map (dict[str, np.ndarray]) : a mapping from context sentence to word embedding\n",
    "        seed (int) : the random generator used in LogisticRegression\n",
    "        \n",
    "    return: Tuple(trained_model, accuracy)\n",
    "        trained_model (sklearn.linear_model.LogisticRegression) : the LR model trained on the train set\n",
    "        accuracy (float) : the accuracy score of the trained model on the test set\n",
    "    \"\"\"\n",
    "    X_train = np.array([np.concatenate((question_embeddings_map[q], context_embeddings_map[s]))\n",
    "                        for q, s in zip(df_train_formatted['question'], df_train_formatted['context_sentence'])])\n",
    "    y_train = df_train_formatted['is_answer_sent'].values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    model = LogisticRegression(random_state=seed, max_iter=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    X_test = np.array([np.concatenate((question_embeddings_map[q], context_embeddings_map[s]))\n",
    "                       for q, s in zip(df_test_formatted['question'], df_test_formatted['context_sentence'])])\n",
    "    y_test = df_test_formatted['is_answer_sent'].values\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return (model, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_get_lr_prediction():\n",
    "    \"Test on the first 1000 rows of the dataset\"\n",
    "    df_squad_train_mini, df_squad_test_mini = train_test_split(df_squad.head(1000), train_size = 0.8, random_state = 0)\n",
    "    df_train_formatted_mini = build_data_for_classification(df_squad_train_mini.reset_index())\n",
    "    df_test_formatted_mini = build_data_for_classification(df_squad_test_mini.reset_index())\n",
    "    lr_mini, acc_mini = get_lr_prediction(\n",
    "        df_train_formatted_mini, df_test_formatted_mini,\n",
    "        question_embeddings_map, context_embeddings_map\n",
    "    )\n",
    "    assert lr_mini.coef_.flatten()[:10].round(2).tolist() == [\n",
    "        0.07, -0.01, -0.03, -0.03, -0.01, 0.01, -0.03, 0.0, 0.04, -0.04\n",
    "    ]\n",
    "    assert lr_mini.intercept_.round(2)[0] == -2.73\n",
    "    assert round(acc_mini, 2) ==  0.81\n",
    "    \n",
    "    \"\"\"Test on the entire dataset\"\"\"\n",
    "    lr, acc = get_lr_prediction(df_train_formatted, df_test_formatted, question_embeddings_map, context_embeddings_map)\n",
    "    assert lr.coef_.flatten()[:10].round(2).tolist() == [\n",
    "        -0.04, 0.0, 0.01, 0.0, -0.0, 0.0, -0.01, -0.0, -0.03, 0.01\n",
    "    ]\n",
    "    assert lr.intercept_.round(2)[0] == -1.54\n",
    "    assert round(acc, 2) == 0.80\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_get_lr_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain about 80% accuracy in this binary classification task, which is not too bad! However, it's important to note that this accuracy cannot be compared with those from previous questions, because it is evaluated in a different setting (`df_test_formatted`). If we were only interested in whether the ground truth answer sentences are correctly detected, we would evaluate the accuracy on the original test set `df_squad_test`, instead of the formatted one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While logistic regression on Sent2vec representation performs relatively well, it still relies on a *uni-directional* representation of words. In this setting, the same word is always mapped to the same vector, even though it may have different meanings in different contexts (e.g., the word `bank` in `bank account` is not the same as in `river bank`). The final model we will explore in this project, which also addresses the above issue, is called BERT (Bi-directional Encoder Representations from Transformers). This is a language model that learns to predict the probability of a sequence of words. The reason for BERT's success is its large feedforward layers and its attention heads, giving it 110 million parameters for the base model and 340 million parameters for the large model. It has been trained on Wikipedia articles and the Book Corpus dataset, which contains text from over 10,000 books of different genres over the tasks of next sentence prediction (NSP) and masked language modeling (MLM). BERT represents the current state of the art in various NLP task, including question answering.\n",
    "\n",
    "One nice feature of NLP models like BERT is that they have already been pre-trained on massive text corpuses, but can also be fine-tuned further for a specific domain (in this case, our SQuAD dataset). We will implement this workflow in the rest of the project. First, we define a sub-class of `Dataset`, similar to Project 6, to turn our SQuaD dataset into a format that PyTorch can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuADDataset(Dataset):\n",
    "    def __init__(self, df_squad):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "        \"\"\"\n",
    "        self.data = df_squad\n",
    "        self.data_cols = [\"question\", \"context_paragraph\", \"answer_start\", \"answer_end\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the dataset length.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get the question, context paragraph, answer start and answer end value\n",
    "        at the row specified by the input index from the dataset.\n",
    "        \"\"\"\n",
    "        return tuple(self.data.loc[index, self.data_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Tokenization for BERT\n",
    "Similar to how our `LogisticRegression` model expects a real-valued vector as input, BERT also has its own way of constructing the input. At a high level, we want to convert each input tuple\n",
    "```\n",
    "(question, context_paragraph, answer_start, answer_end)\n",
    "```\n",
    "into a tuple\n",
    "```\n",
    "(encoding, token_start, token_end)\n",
    "```\n",
    "where `encoding` is a dictionary that maps three keywords -- `\"input_ids\"`, `\"token_type_ids\"`, `\"attention_mask\"` -- to their respective vector representations.\n",
    "\n",
    "Implement the class `SQuADTokenizer` that performs the above conversion using a pre-trained Bert tokenizer. In particular, the class constructor accepts a `BertTokenizer` instance and the maximum sequence size `max_length`, which are stored as instance variables. Then, the `__call__` function acccepts a batch of data and performs the following steps:\n",
    "1. Extract the questions, context paragraphs, answer start indexes, and answer end indexes from the batch.\n",
    "1. Input the questions and contexts to the tokenizer as separate lists, so that the tokenizer can append special tokens such as `[SEP]` that help BERT recognize different passages. You should also set the following parameters: `padding` to `\"longest\"`, `truncation` to `True`, `max_length` to the stored `max_len`, and `return_tensors` to `\"pt\"`.\n",
    "1. Convert the `answer_start` and `answer_end` character indices to the indices of the two tokens that correspond to these start and end characters. For example, let's say:\n",
    "```py\n",
    "question = \"Which pets does your son have?\"\n",
    "context_paragraph = \"My son loves pets. He has two cats and a dog.\"\n",
    "tokenized_context = [\"my\", \"son\", \"love\", \"pet\", \"he\", \"have\", \"two\", \"cat\", \"and\", \"a\", \"dog\"]\n",
    "answer_start, answer_end = 26, 44 # answer = \"two cats and a dog\"\n",
    "```\n",
    "now you need to identify the token that corresponds to the character `context_paragraph[answer_start]`. This character is `t` and the corresponding token is `two`, which is at index `6` in `tokenized_context`. Similarly for `answer_end`, the character `context_paragraph[answer_end]` is `g`, and the corresponding token is `dog`, which is at index 10 in `tokenized_context`. In summary, you are converting\n",
    "```\n",
    "(answer_start = 26, answer_end = 44)\n",
    "```\n",
    "to\n",
    "```\n",
    "(token_start = 6, token_end = 10)\n",
    "```\n",
    "1. Return the encoded data (output from calling the tokenizer on the input questions and context paragraphs in Step 2), as well as the list of token start indexes and the list of token end indexes.\n",
    "\n",
    "**Notes**:\n",
    "* The [Tokenizer page](https://huggingface.co/transformers/main_classes/tokenizer.html) and the section about [using the tokenizer](https://huggingface.co/transformers/quicktour.html?highlight=max_length) on HuggingFace may be helpful.\n",
    "* To convert `(answer_start, answer_end)` to `(token_start, token_end)`, you can use the [.char_to_token](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.BatchEncoding.char_to_token) method. Here the `batch_or_char_index` is the index of the current data point within the input batch, and `sequence_index` should be 1 because we are doing this conversion on the context paragraph, which is the second input string to the tokenizer. You need to specify both `batch_or_char_index` and `char_index` when calling `.char_to_token`.\n",
    "* Sometimes calling `.char_to_token` will return `None` because the tokens have been changed (e.g., due to lemmatization) from their original form in `context_paragraph`. A simple work-around is to also consider the token that corresponds to the next or previous character. If doing so still yields `None`, we will just set the token index as `max_len` as a last resort. More formally, you should assign `token_start` to the first element that is not `None` among the following four values:\n",
    "    * `char_to_token(char_index = answer_start, ...)`\n",
    "    * `char_to_token(char_index = answer_start+1, ...)`\n",
    "    * `char_to_token(char_index = answer_start-1, ...)`\n",
    "    * `max_len`\n",
    "* You will do a similar assignment for `token_end` as well.\n",
    "* After extracting `answer_starts` and `anwer_ends` from the batch, you should convert them to `LongTensor` so that they can be used in `char_to_token()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuADTokenizer:\n",
    "    def __init__(self, tokenizer, max_len = 512):\n",
    "        \"\"\"\n",
    "        Store the input BertTokenizer instance and the max length\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Perform tokenization on a batch of data\n",
    "        \n",
    "        args:\n",
    "            batch (Tuple[questions, contexts, answer_starts, answer_ends]):\n",
    "                questions (List[str]) : a list of questions\n",
    "                contexts (List[str]) : a list of context paragraphs\n",
    "                answer_starts (List[int]) : a list of answer start indexes\n",
    "                answer_ends (List[int]) : a list of answer end indexes\n",
    "        \n",
    "        returns:\n",
    "            Tuple[encoding, token_starts, token_ends]\n",
    "                encoding (dict[str, tensor]): the output of calling tokenizer on the questions and contexts\n",
    "                token_starts (List[int]) : the list of indexes for the tokens that correspond to the first answer character\n",
    "        \"\"\"\n",
    "        question, context, answer_start, answer_end = batch\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "                            question,\n",
    "                            context,\n",
    "                            padding=\"longest\",\n",
    "                            truncation=True,\n",
    "                            max_length=self.max_len,\n",
    "                            return_tensors=\"pt\")\n",
    "        \n",
    "        answer_start = torch.LongTensor(answer_start)\n",
    "        answer_end = torch.LongTensor(answer_end)\n",
    "        \n",
    "        start_token = []\n",
    "        end_token = []\n",
    "        \n",
    "        for i in range(len(question)):\n",
    "            start_token.append(self._get_token_index(i, answer_start[i], encoding))\n",
    "            end_token.append(self._get_token_index(i, answer_end[i], encoding))\n",
    "            \n",
    "        return encoding, start_token, end_token\n",
    "    \n",
    "    def _get_token_index(self, batch_index, char_index, encoding):\n",
    "        token_index = encoding.char_to_token(batch_or_char_index=batch_index, char_index=char_index, sequence_index=1)\n",
    "    \n",
    "        if token_index is None:\n",
    "            for offset in [0, 1, -1]:\n",
    "                token_index = encoding.char_to_token(batch_or_char_index=batch_index, char_index=char_index + offset, sequence_index=1)\n",
    "                if token_index is not None:\n",
    "                    return token_index\n",
    "\n",
    "        if token_index is None:\n",
    "            token_index = self.max_len\n",
    "\n",
    "        return token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_tokenizer():\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    batch_tokenizer = SQuADTokenizer(tokenizer)\n",
    "    example_1 = (\n",
    "        ['When did Beyonce start becoming popular?'],\n",
    "        ['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'],\n",
    "        [269],\n",
    "        [286]\n",
    "    )\n",
    "    encoding, token_starts, token_ends = batch_tokenizer(example_1)\n",
    "    check_equal(list(encoding[\"input_ids\"].shape), [1, 174])\n",
    "    check_equal(encoding[\"input_ids\"][0][:10].numpy().tolist(), [\n",
    "        101, 2043, 2106, 20773, 2707, 3352, 2759, 1029, 102, 20773\n",
    "    ])\n",
    "    check_equal(encoding[\"token_type_ids\"].numpy().tolist(), [[0]*9 + [1]*165])\n",
    "    check_equal(encoding[\"attention_mask\"].numpy().tolist(), [[1] * 174])\n",
    "    check_equal(token_starts, [75])\n",
    "    check_equal(token_ends, [79])\n",
    "\n",
    "    example_2 = (\n",
    "        ['When did Beyonce start becoming popular?', 'What score did the writer from the Chicago Tribune give to Spectre?'], \n",
    "        ['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'Critical appraisal of the film was mixed in the United States. In a lukewarm review for RogerEbert.com, Matt Zoller Seitz gave the film 2.5 stars out of 4, describing Spectre as inconsistent and unable to capitalise on its potential. Kenneth Turan, reviewing the film for Los Angeles Times, concluded that Spectre \"comes off as exhausted and uninspired\". Manohla Dargis of The New York Times panned the film as having \"nothing surprising\" and sacrificing its originality for the sake of box office returns. Forbes\\' Scott Mendelson also heavily criticised the film, denouncing Spectre as \"the worst 007 movie in 30 years\". Darren Franich of Entertainment Weekly viewed Spectre as \"an overreaction to our current blockbuster moment\", aspiring \"to be a serialized sequel\" and proving \"itself as a Saga\". While noting that \"[n]othing that happens in Spectre holds up to even minor logical scrutiny\", he had \"come not to bury Spectre, but to weirdly praise it. Because the final act of the movie is so strange, so willfully obtuse, that it deserves extra attention.\" In a positive review Rolling Stone, Peter Travers gave the film 3.5 stars out of 4, describing \"The 24th movie about the British MI6 agent with a license to kill is party time for Bond fans, a fierce, funny, gorgeously produced valentine to the longest-running franchise in movies\". Other positive reviews from Mick LaSalle from the San Francisco Chronicle, gave it a perfect 100 score, stating: “One of the great satisfactions of Spectre is that, in addition to all the stirring action, and all the timely references to a secret organization out to steal everyone’s personal information, we get to believe in Bond as a person.” Stephen Whitty from the New York Daily News, gave it an 80 grade, saying: “Craig is cruelly efficient. Dave Bautista makes a good, Oddjob-like assassin. And while Lea Seydoux doesn’t leave a huge impression as this film’s “Bond girl,” perhaps it’s because we’ve already met — far too briefly — the hypnotic Monica Bellucci, as the first real “Bond woman” since Diana Rigg.” Richard Roeper from the Chicago Sun-Times, gave it a 75 grade. He stated: “This is the 24th Bond film and it ranks solidly in the middle of the all-time rankings, which means it’s still a slick, beautifully photographed, action-packed, international thriller with a number of wonderfully, ludicrously entertaining set pieces, a sprinkling of dry wit, myriad gorgeous women and a classic psycho-villain who is clearly out of his mind but seems to like it that way.” Michael Phillips over at the Chicago Tribune, gave it a 75 grade. He stated: “For all its workmanlike devotion to out-of-control helicopters, “Spectre” works best when everyone’s on the ground, doing his or her job, driving expensive fast cars heedlessly, detonating the occasional wisecrack, enjoying themselves and their beautiful clothes.” Guy Lodge from Variety, gave it a 70 score, stating: “What’s missing is the unexpected emotional urgency of “Skyfall,” as the film sustains its predecessor’s nostalgia kick with a less sentimental bent.”'], \n",
    "        [269, 2118], \n",
    "        [286, 2120]\n",
    "    )\n",
    "    encoding, token_starts, token_ends = batch_tokenizer(example_2)\n",
    "    check_equal(list(encoding[\"input_ids\"].shape), [2, 512])\n",
    "    check_equal(encoding[\"input_ids\"][0][-10:].numpy().tolist(), [0]*10)\n",
    "    check_equal(encoding[\"token_type_ids\"].numpy().tolist()[0], [0]*9 + [1]*165 + [0]*338)\n",
    "    check_equal(encoding[\"token_type_ids\"].numpy().tolist()[1], [0]*16 + [1]*496)\n",
    "    check_equal(encoding[\"attention_mask\"][0].numpy().tolist(), [1]*174 + [0]*338)\n",
    "    check_equal(token_starts, [75, 512])\n",
    "    check_equal(token_ends, [79, 512])\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Fine-tuning BERT\n",
    "With the tokenizer ready, we can begin fine-tuning a pre-trained BERT model to our dataset with the following steps:\n",
    "1. Move the input `model` to `device` and set it to training mode.\n",
    "1. Repeat `n_iters` times:\n",
    "    * For every batch of data from the dataloader:\n",
    "        * Use the input `tokenizer` (an instance of the class `SQuADTokenizer` that you implemented) to encode this batch, yielding the tuple `(encoding, token_starts, token_ends)`.\n",
    "        * Extract the `input_ids`, `token_type_ids` and `attention_mask` from `encoding`. Convert these tensors to `LongTensor` and move them to `device`.\n",
    "        * Perform the usual PyTorch training workflow (zero grad, forward pass, backprop, ...). See the PyTorch primer for a reminder.\n",
    "        \n",
    "Implement the function `fine_tune_bert` that, given a pretrained BERT model and other training parameters, performs the above training procedure. This function should return the fine-tuned model and the average training loss across epochs.\n",
    "\n",
    "**Notes**:\n",
    "* Consult the [Bert documentation page](https://huggingface.co/transformers/v4.7.0/model_doc/bert.html#transformers.BertForQuestionAnswering.forward) for which parameters to specify in the forward pass. Make sure to input them in the correct order, as specified in the documentation.\n",
    "* To carry out the forward pass, you can input the encoding elements (after convering them to datatype Long), along with `token_starts` and `token_ends`, to `model`. Calling `.loss` on the output of the forward pass will yield the loss value.\n",
    "* We also provide a `verbose` flag. If you would like to add print debugging messages during model training, simply precede each print statement with an `if verbose` check, for example:\n",
    "```py\n",
    "if verbose:\n",
    "    print(\"Training loss\", train_loss)\n",
    "```\n",
    "The autograder will only call your function with `verbose = False`, so that your printout messages do not interfere with grading.\n",
    "* To compute the average training loss, you should sum all the losses from every forward pass, then divide this sum by the number of epochs at the end of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def fine_tune_bert(model, n_epochs, optimizer, dataloader, squad_tokenizer, device, verbose = False):\n",
    "    \"\"\"\n",
    "    Fine-tune a pre-trained BERT model on the SQuAD dataset.\n",
    "    \n",
    "    args:\n",
    "        model (BertForQuestionAnswering) : a pre-trained BERT model for QA tasks\n",
    "        n_epochs (int) : the number of epochs to train\n",
    "        dataloader (DataLoader) : a data loader that provides access to one batch of data at a time\n",
    "        squad_tokenizer (SQuADTokenizer) : a tokenizer instance to be called on every batch of data from dataloader\n",
    "        device (torch.device) : the device (CPU or Cuda) that the model and data should be moved to\n",
    "        verbose (bool) : a flag that indicates whether debug messages should be printed out\n",
    "    \n",
    "    return:\n",
    "        model (BertForQuestionAnswering) : the fine-tuned model\n",
    "        avg_loss (float) : the average training loss across epochs\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            encoding, token_starts, token_ends = squad_tokenizer(batch)\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            token_type_ids = encoding['token_type_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            start_positions = torch.LongTensor(token_starts).to(device)\n",
    "            end_positions = torch.LongTensor(token_ends).to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                            start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        total_loss += avg_epoch_loss\n",
    "\n",
    "    avg_loss = total_loss / n_epochs\n",
    "\n",
    "    return model, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "All tests passed!\n",
      "CPU times: user 6.71 s, sys: 764 ms, total: 7.47 s\n",
      "Wall time: 9.52 s\n"
     ]
    }
   ],
   "source": [
    "def test_fine_tune_bert():\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    squad_tokenizer = SQuADTokenizer(tokenizer)\n",
    "    \n",
    "    \"\"\"Train on 8 data points\"\"\"\n",
    "    train_dataset = SQuADDataset(df_squad.head(8)[[\"question\", \"context_paragraph\", \"answer_start\", \"answer_end\"]])\n",
    "    squad_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False, num_workers=6, worker_init_fn=_init_fn)\n",
    "    model = BertForQuestionAnswering.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)  \n",
    "    model, avg_train_loss = fine_tune_bert(model, 1, optimizer, squad_dataloader, squad_tokenizer, device)\n",
    "    assert avg_train_loss < 5.0, avg_train_loss\n",
    "    \n",
    "    \"\"\"Train on 100 data points\"\"\"\n",
    "    train_dataset = SQuADDataset(df_squad.head(100)[[\"question\", \"context_paragraph\", \"answer_start\", \"answer_end\"]])\n",
    "    squad_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False, num_workers=6, worker_init_fn=_init_fn)\n",
    "    model = BertForQuestionAnswering.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)  \n",
    "    model, avg_train_loss = fine_tune_bert(model, 1, optimizer, squad_dataloader, squad_tokenizer, device)\n",
    "    assert avg_train_loss < 38, avg_train_loss\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "%time test_fine_tune_bert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fine-tune the model on 80% of the data (the remaining 20% will be for evaluation). Because this takes a long time, we will only do it for 1 epoch.\n",
    "\n",
    "**Notes**:\n",
    "* The following cell will take about **1 hour** to run, and is **required** for the next question. We recommend that you make a submission to Sail() at this point, to make sure you have everything correct so far. Ideally you would want to avoid having to fine-tune BERT more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "CPU times: user 59min 52s, sys: 12.6 s, total: 1h 4s\n",
      "Wall time: 59min 15s\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "train_indexes, test_indexes = train_test_split(df_squad.index, train_size = 0.8, random_state = 0)\n",
    "df_squad_train = df_squad.loc[train_indexes, [\"question\", \"context_paragraph\", \"answer_start\", \"answer_end\"]].reset_index()\n",
    "df_squad_test = df_squad.loc[test_indexes].reset_index()\n",
    "\n",
    "train_dataset = SQuADDataset(df_squad_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=False, num_workers=6, worker_init_fn=_init_fn, pin_memory = True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "squad_tokenizer = SQuADTokenizer(tokenizer)\n",
    "\n",
    "%time tuned_model, avg_train_loss = fine_tune_bert(model, 1, optimizer, train_dataloader, squad_tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also save the fine-tuned model into a directory to reuse it later. Note that the following code will create a directory `bert_fine_tuned_squad` with several files in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "tuned_model.cpu().save_pretrained(\"bert_fine_tuned_squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the fine tuned model performs. We provide the following function `get_bert_prediction` to predict the answer given a pair of question and context. Overall the inference process is very similar to the forward pass during fine-tuning, except that we don't provide the starting and ending tokens to the Bert model, since those are what we need to predict.\n",
    "\n",
    "You may also notice that we are looping through each data point, instead of doing inference in a vectorized manner. The reason is that this same inference code will be used for model deployment on CPU later, where we have limited memory and cannot afford to process data in batches (recall how you performed inference both in the loop approach and the batch approach in the OPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_prediction(questions, contexts, device, model=None, tokenizer=None):\n",
    "    '''\n",
    "    Given a list of questions and a list of corresponding contexts, predict the answers using BERT.\n",
    "\n",
    "    args:\n",
    "        questions (List[string]): list of questions to be answered\n",
    "        contexts (List[string]): list of context paragraphs, each for answering a question in the input questions\n",
    "        device (torch.device) : the device (CPU or Cuda) that the model and data should be moved to\n",
    "        model (BertForQuestionAnswering): BERT model to be used for question answering \n",
    "            or None - if None, `bertserini-bert-base-squad` will be loaded\n",
    "        tokenizer (BertTokenizerFast object): tokenizer to be used for encoding questions and contexts\n",
    "            or None - if None, `bertserini-bert-base-squad` will be loaded\n",
    "    return:\n",
    "        outputs (List[string]): list of generated answers\n",
    "    '''\n",
    "    \n",
    "    if model is None:\n",
    "        model = BertForQuestionAnswering.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    if tokenizer is None:\n",
    "        tokenizer = BertTokenizerFast.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for question, context in tqdm(zip(questions, contexts), total=len(questions)):\n",
    "\n",
    "        encoded_seq = tokenizer(question, context, padding=\"longest\", truncation=True, max_length=512)\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_seq[\"input_ids\"])\n",
    "        \n",
    "        input_ids = torch.LongTensor([encoded_seq[\"input_ids\"]]).to(device)\n",
    "        token_type_ids = torch.LongTensor([encoded_seq[\"token_type_ids\"]]).to(device)\n",
    "        attention_mask = torch.FloatTensor([encoded_seq[\"attention_mask\"]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, \n",
    "                           attention_mask=attention_mask, \n",
    "                           token_type_ids=token_type_ids)\n",
    "        logits_start, logits_end = output['start_logits'], output['end_logits']\n",
    "        token_start = torch.argmax(logits_start)\n",
    "        token_end = torch.argmax(logits_end)\n",
    "        \n",
    "        outputs.append(tokenizer.convert_tokens_to_string(tokens[token_start:token_end]))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try predicting one row of data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      "How many awards was Beyonce nominated for at the 52nd Grammy Awards?\n",
      "context:\n",
      "At the 52nd Annual Grammy Awards, Beyoncé received ten nominations, including Album of the Year for I Am... Sasha Fierce, Record of the Year for \"Halo\", and Song of the Year for \"Single Ladies (Put a Ring on It)\", among others. She tied with Lauryn Hill for most Grammy nominations in a single year by a female artist. In 2010, Beyoncé was featured on Lady Gaga's single \"Telephone\" and its music video. The song topped the US Pop Songs chart, becoming the sixth number-one for both Beyoncé and Gaga, tying them with Mariah Carey for most number-ones since the Nielsen Top 40 airplay chart launched in 1992. \"Telephone\" received a Grammy Award nomination for Best Pop Collaboration with Vocals.\n",
      "answer:\n",
      "ten\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 49.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert prediction: ['ten']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"question:\\n{}\\ncontext:\\n{}\\nanswer:\\n{}\\n\".format(\n",
    "    df_squad.loc[200, \"question\"],\n",
    "    df_squad.loc[200, \"context_paragraph\"],\n",
    "    df_squad.loc[200, \"answer\"]\n",
    "))\n",
    "\n",
    "print(\"Bert prediction:\", get_bert_prediction(\n",
    "    df_squad.loc[[200], \"question\"],\n",
    "    df_squad.loc[[200], \"context_paragraph\"],\n",
    "    device,\n",
    "    model = tuned_model\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it does work! Now let's see how accurate the fine tuned model is on the entire test set. We'll also compute the accuracy of a pre-trained model without fine-tuning, to see how much improvement our fine-tuning provided. Each of the following two cells will take about **11 minutes** to run, and they are **not** required for later questions, so feel free to skip them for now and revisit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17365/17365 [03:57<00:00, 73.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008407716671465592\n"
     ]
    }
   ],
   "source": [
    "pretrained_test_prediction = get_bert_prediction(\n",
    "    df_squad_test[\"question\"], df_squad_test[\"context_paragraph\"],\n",
    "    device, BertForQuestionAnswering.from_pretrained('rsvp-ai/bertserini-bert-base-squad')\n",
    ")\n",
    "print( (np.array(pretrained_test_prediction) == df_squad_test[\"answer\"].str.lower().values ).mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17365/17365 [03:57<00:00, 73.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5823207601497264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tuned_test_predictions = get_bert_prediction(\n",
    "    df_squad_test[\"question\"], df_squad_test[\"context_paragraph\"],\n",
    "    device, tuned_model\n",
    ")\n",
    "print( (np.array(tuned_test_predictions) == df_squad_test[\"answer\"].str.lower().values ).mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the pretrained model did not work well at all, while fine-tuning the model for one epoch already yields a large improvement in accuracy (about 58%) on the test set. Naturally, the model still has plenty of room for improvement when trained for more epochs, but doing so requires a lot more time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Deployment\n",
    "As seen from this project, building a QA system involves a very complex technology stack. To make your model easily accessible to others, you can deploy it to a public endpoint on Azure, using the same workflow from Project 6. We have built several models so far and they can all be deployed together under the same endpoint. However, for the rest of this project, let's focus on BERT deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.webservice import LocalWebservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide a brief reminder of the involved steps. You can consult the code from your Project 6 or the [model deployment primer](https://nbviewer.jupyter.org/url/clouddatascience.blob.core.windows.net/primers/p5-machine-learning-azure-primer/azure_model_deployment_primer.ipynb) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Initialize workspace with `Workspace.from_config()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Workspace name: project7\n",
      "Azure region: eastus\n",
      "Subscription id: 66f84b8e-f976-4ebd-9d1e-bb78b3b9002c\n",
      "Resource group: p7\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Register the model with `Model.register()`**\n",
    "\n",
    "The `model_path` parameter should be set to the name of your Bert model directory, `\"./bert_fine_tuned_squad\"`. The `model_name` parameter should be set to `\"bert_fine_tuned\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model bert_fine_tuned\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "register_fine_tuned_bert_model = Model.register(\n",
    "    workspace = ws, model_path = \"./bert_fine_tuned_squad\",\n",
    "    model_name = \"bert_fine_tuned\",\n",
    "    description = \"Fine tuned BERT model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Create scoring script and API endpoint**\n",
    "\n",
    "Our grader will send a POST request to your endpoint, where the JSON content is structured as follows:\n",
    "\n",
    "```py\n",
    "{\n",
    "    \"questions\" : [\n",
    "        \"How many parameters does BERT-large have?\",\n",
    "        \"When did Beyonce start becoming popular?\"\n",
    "    ],\n",
    "\n",
    "    \"context_paragraphs\" : [\n",
    "        \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\",\n",
    "        'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'\n",
    "    ]\n",
    "}\n",
    "```\n",
    "Here `questions` is a list of `N` questions and `context_paragraphs` is a list of `N` context paragraphs. For each pair of question and context paragraph, you should tokenize and input them to the fine-tuned Bert model, and return a dictionary with the following format:\n",
    "```py\n",
    "{\n",
    "    'predicted_ans' : ['340', 'late 1990s']\n",
    "}\n",
    "```\n",
    "where the key `predicted_ans` maps to a list of `N` strings, with each string being the predicted answer for one input pair of question and context paragraph.\n",
    "\n",
    "Implement the `init` and `run` function in the `score.py` file to perform the above processing. In particular,\n",
    "* `init` will load the fine-tuned Bert model from file and store it in a global variable.\n",
    "* `run` will convert the input `input_data` to JSON, extract the questions and contexts, then perform inference and return the specified response. You can reuse the code from `get_bert_predictions` that we provided earlier.\n",
    "\n",
    "**Notes**:\n",
    "* To load the fine tuned Bert model, you can call\n",
    "```py\n",
    "BertForQuestionAnswering.from_pretrained(Model.get_model_path(\"bert_fine_tuned\"))\n",
    "```\n",
    "Also remember that to modify a global variable, you need to use the `global` keyword.\n",
    "* When doing inference you can use the pre-trained tokenizer `\"rsvp-ai/bertserini-bert-base-squad\"`. This tokenizer should be created in `init` so that you don't need to take time loading it during inference.\n",
    "* Keep in mind that the return value of `run` in `scoring.py` should be a dictionary, not a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import os, json, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertForQuestionAnswering\n",
    "from azureml.core.model import Model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def init():\n",
    "    \"\"\"\n",
    "    Load the fine-tuned Bert model from file and store it in a global variable.\n",
    "    Also initialize the pre-trained tokenizer.\n",
    "    \"\"\"\n",
    "    global model\n",
    "    global tokenizer\n",
    "    model_path = Model.get_model_path(\"bert_fine_tuned\")\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"rsvp-ai/bertserini-bert-base-squad\")\n",
    "\n",
    "def run(input_data):\n",
    "    \"\"\"\n",
    "    Convert the input data from string to JSON, extract the questions and contexts,\n",
    "    then perform inference with Bert and return the specified JSOn response \n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(input_data)\n",
    "        questions = data['questions']\n",
    "        context_paragraphs = data['context_paragraphs']\n",
    "\n",
    "        predicted_ans = []\n",
    "\n",
    "        for question, context in zip(questions, context_paragraphs):\n",
    "            inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = inputs[\"input_ids\"].to(device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                start_logits = outputs.start_logits\n",
    "                end_logits = outputs.end_logits\n",
    "\n",
    "            all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "            answer = tokenizer.convert_tokens_to_string(all_tokens[torch.argmax(start_logits) : torch.argmax(end_logits) + 1])\n",
    "            predicted_ans.append(answer.strip())\n",
    "\n",
    "        return {\"predicted_ans\": predicted_ans}\n",
    "    \n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return {\"error\": error}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check that the content of `score.py` is as you expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "import os, json, pickle\r\n",
      "import numpy as np\r\n",
      "import torch\r\n",
      "from tqdm import tqdm\r\n",
      "from transformers import BertTokenizerFast\r\n",
      "from transformers import BertForQuestionAnswering\r\n",
      "from azureml.core.model import Model\r\n",
      "\r\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
      "model = None\r\n",
      "tokenizer = None\r\n",
      "\r\n",
      "def init():\r\n",
      "    \"\"\"\r\n",
      "    Load the fine-tuned Bert model from file and store it in a global variable.\r\n",
      "    Also initialize the pre-trained tokenizer.\r\n",
      "    \"\"\"\r\n",
      "    global model\r\n",
      "    global tokenizer\r\n",
      "    model_path = Model.get_model_path(\"bert_fine_tuned\")\r\n",
      "    model = BertForQuestionAnswering.from_pretrained(model_path)\r\n",
      "    model.to(device)\r\n",
      "    tokenizer = BertTokenizerFast.from_pretrained(\"rsvp-ai/bertserini-bert-base-squad\")\r\n",
      "\r\n",
      "def run(input_data):\r\n",
      "    \"\"\"\r\n",
      "    Convert the input data from string to JSON, extract the questions and contexts,\r\n",
      "    then perform inference with Bert and return the specified JSOn response \r\n",
      "    \"\"\"\r\n",
      "    try:\r\n",
      "        data = json.loads(input_data)\r\n",
      "        questions = data['questions']\r\n",
      "        context_paragraphs = data['context_paragraphs']\r\n",
      "\r\n",
      "        predicted_ans = []\r\n",
      "\r\n",
      "        for question, context in zip(questions, context_paragraphs):\r\n",
      "            inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", padding=True, truncation=True)\r\n",
      "            input_ids = inputs[\"input_ids\"].to(device)\r\n",
      "            attention_mask = inputs[\"attention_mask\"].to(device)\r\n",
      "\r\n",
      "            with torch.no_grad():\r\n",
      "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\r\n",
      "                start_logits = outputs.start_logits\r\n",
      "                end_logits = outputs.end_logits\r\n",
      "\r\n",
      "            all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\r\n",
      "            answer = tokenizer.convert_tokens_to_string(all_tokens[torch.argmax(start_logits) : torch.argmax(end_logits) + 1])\r\n",
      "            predicted_ans.append(answer.strip())\r\n",
      "\r\n",
      "        return {\"predicted_ans\": predicted_ans}\r\n",
      "    \r\n",
      "    except Exception as e:\r\n",
      "        error = str(e)\r\n",
      "        return {\"error\": error}\r\n"
     ]
    }
   ],
   "source": [
    "!cat score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Create environment file**\n",
    "\n",
    "Now you will need to create an environment file (`myenv.yml`) that specifies all of the scoring script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image by Azure ML. The `pip_packages` parameter value should be the following list:\n",
    "```py\n",
    "['azureml-defaults', 'torch==2.0.1', 'transformers==4.25.0', 'numpy']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Conda environment specification. The dependencies defined in this file will\r\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\r\n",
      "\n",
      "# Details about the Conda environment file format:\r\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\r\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\r\n",
      "  # Currently Azure ML only supports 3.8 and later.\r\n",
      "- python=3.8.13\n",
      "\n",
      "- pip:\n",
      "  - azureml-defaults\n",
      "  - torch==2.0.1\n",
      "  - transformers==4.25.0\n",
      "  - numpy\n",
      "channels:\n",
      "- anaconda\n",
      "- conda-forge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "environment_file = CondaDependencies.create(pip_packages=[\n",
    "    'azureml-defaults', 'torch==2.0.1', 'transformers==4.25.0', 'numpy'\n",
    "])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(environment_file.serialize_to_string())\n",
    "\n",
    "print(environment_file.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Deploy to local service**\n",
    "\n",
    "Follow the steps in the [model deloyment primer](https://nbviewer.jupyter.org/url/clouddatascience.blob.core.windows.net/primers/machine-learning-azure-primer/azure_model_deployment_primer.ipynb) to create an `Environment`, an `InferenceConfig`, a `LocalWebservice`, a `Model.deploy` object, and call `wait_for_deployment` on it. This code may take about 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5814/485310879.py:9: FutureWarning: azureml.core.model:\n",
      "To leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \n",
      "please refer to respective documentations \n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \n",
      "For more information on migration, see https://aka.ms/acimoemigration \n",
      "To disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n",
      "  bert_local_service = Model.deploy(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model bert_fine_tuned:1 to /tmp/azureml_zdd3qx8k/bert_fine_tuned/1\n",
      "Generating Docker build context.\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry c3059e80e90645c7aa08791d2d49daff.azurecr.io\n",
      "Logging into Docker registry c3059e80e90645c7aa08791d2d49daff.azurecr.io\n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/5 : FROM c3059e80e90645c7aa08791d2d49daff.azurecr.io/azureml/azureml_4fcb95632e97c3be0ee438ce3ff8c40e\n",
      " ---> 2cee74f97f4c\n",
      "Step 2/5 : COPY azureml-app /var/azureml-app\n",
      " ---> 5faff0bef579\n",
      "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjY2Zjg0YjhlLWY5NzYtNGViZC05ZDFlLWJiNzhiM2I5MDAyYyIsInJlc291cmNlR3JvdXBOYW1lIjoicDciLCJhY2NvdW50TmFtZSI6InByb2plY3Q3Iiwid29ya3NwYWNlSWQiOiJjMzA1OWU4MC1lOTA2LTQ1YzctYWEwOC03OTFkMmQ0OWRhZmYifSwibW9kZWxzIjp7fSwibW9kZWxzSW5mbyI6e319 | base64 --decode > /var/azureml-app/model_config_map.json\n",
      " ---> Running in 05d69ded0c41\n",
      " ---> f516d7ae812c\n",
      "Step 4/5 : RUN mv '/var/azureml-app/tmpq7lp_n00.py' /var/azureml-app/main.py\n",
      " ---> Running in fd10f09b5c80\n",
      " ---> 1cdd5e1f2198\n",
      "Step 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n",
      " ---> Running in 973582bd0564\n",
      " ---> 4e5ab3262145\n",
      "Successfully built 4e5ab3262145\n",
      "Successfully tagged bert-local-service:latest\n",
      "Container has been successfully cleaned up.\n",
      "Image sha256:5803f18b2f8bff19e0b11014041d17fc95a3f680bde0c1fbe8d6945dbe12b344 successfully removed.\n",
      "Starting Docker container...\n",
      "Docker container running.\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "myenv = Environment.from_conda_specification(name = \"myenv\", file_path = \"myenv.yml\")\n",
    "myenv.register(workspace = ws)\n",
    "\n",
    "inference_config = InferenceConfig(source_directory = '.', entry_script = \"score.py\", environment = myenv)\n",
    "\n",
    "local_deployment_target = LocalWebservice.deploy_configuration(port = 8890)\n",
    "\n",
    "bert_local_service = Model.deploy(\n",
    "    ws, \"bert-local-service\",\n",
    "    [register_fine_tuned_bert_model], inference_config, \n",
    "    local_deployment_target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Test the local service**\n",
    "\n",
    "You should change the `local_service` variable below to the variable name that you used earlier to store the return value of `Model.deploy`. Check that the response JSON matches the format specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking container health...\n",
      "Local webservice is running at http://localhost:8890\n",
      "{'predicted_ans': ['340m parameters', \"late 1990s as lead singer of r & b girl - group destiny ' s child. managed by her father, mathew knowles, the group became one of the world ' s best - selling girl groups of all time. their hiatus saw the release of beyonce ' s debut album, dangerously in love ( 2003 )\"]}\n"
     ]
    }
   ],
   "source": [
    "input_json = json.dumps({\n",
    "    \"questions\" : [\n",
    "        \"How many parameters does BERT-large have?\",\n",
    "        \"When did Beyonce start becoming popular?\"\n",
    "    ],\n",
    "\n",
    "    \"context_paragraphs\" : [\n",
    "        \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\",\n",
    "        'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'\n",
    "    ]\n",
    "})\n",
    "\n",
    "input_json = bytes(input_json, encoding = \"utf8\")\n",
    "output = bert_local_service.run(input_json)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your model may output `340` or `340m` as the answer to the first question -- both are acceptable, due to the random weights in the Bert model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Deploy to ACI container**\n",
    "\n",
    "Now that your local service has worked properly, the last step is to deploy it to a pulic endpoint. Follow the steps in the primer to create an `AciWebService` and a new `Model.deploy` object, then call `wait_for_deployment` on it. This code should take about 20 minutes to run. If it takes longer, you should consult the \"Monitoring public deployment\" section in the primer to diagnose the issues.\n",
    "\n",
    "**Notes**:\n",
    "* When creating the `AciWebService`, you should specify `cpu_cores = 3.8` and `memory_gb = 15` to maximize processing power in the deployment environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5814/2045019769.py:7: FutureWarning: azureml.core.model:\n",
      "To leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \n",
      "please refer to respective documentations \n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \n",
      "For more information on migration, see https://aka.ms/acimoemigration \n",
      "To disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n",
      "  bert_aci_service = Model.deploy(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2024-04-22 03:51:14+00:00 Creating Container Registry if not exists.\n",
      "2024-04-22 03:51:14+00:00 Registering the environment.\n",
      "2024-04-22 03:51:14+00:00 Use the existing image.\n",
      "2024-04-22 03:51:14+00:00 Generating deployment configuration..\n",
      "2024-04-22 03:51:29+00:00 Submitting deployment to compute..\n",
      "2024-04-22 03:51:36+00:00 Checking the status of deployment bert-fine-tuned-aci-service..\n",
      "2024-04-22 03:55:21+00:00 Checking the status of inference endpoint bert-fine-tuned-aci-service.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "aci_deployment_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=3.8, memory_gb=15, description = 'Public endpoint for Fine Tuned BERT model',\n",
    "    tags={'data' : 'SQuAD dataset', 'model' : 'BERT', 'fine_tuned' : 'YES'},\n",
    ")\n",
    "\n",
    "bert_aci_service = Model.deploy(\n",
    "    ws, \"bert-fine-tuned-aci-service\",\n",
    "    [register_fine_tuned_bert_model], inference_config, \n",
    "    aci_deployment_config\n",
    ")\n",
    "\n",
    "bert_aci_service.wait_for_deployment(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8: Test the public service**\n",
    "\n",
    "Let's use the same input json from earlier and check that the public endpoint is working as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://fd496440-ce43-407b-9b05-7ca1b3f83eea.eastus.azurecontainer.io/score\n",
      "200\n",
      "{'predicted_ans': ['340m parameters', \"late 1990s as lead singer of r & b girl - group destiny ' s child. managed by her father, mathew knowles, the group became one of the world ' s best - selling girl groups of all time. their hiatus saw the release of beyonce ' s debut album, dangerously in love ( 2003 )\"]}\n"
     ]
    }
   ],
   "source": [
    "deployed_uri = bert_aci_service.scoring_uri\n",
    "print(deployed_uri)\n",
    "\n",
    "input_json = json.dumps({\n",
    "    \"questions\" : [\n",
    "        \"How many parameters does BERT-large have?\",\n",
    "        \"When did Beyonce start becoming popular?\"\n",
    "    ],\n",
    "\n",
    "    \"context_paragraphs\" : [\n",
    "        \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\",\n",
    "        'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'\n",
    "    ]\n",
    "})\n",
    "\n",
    "response = requests.post(deployed_uri, input_json, headers = {'Content-Type' : 'application/json'})\n",
    "print(response.status_code)\n",
    "print(json.loads(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Deploying Bert model\n",
    "We provide a local test to check that your deployed model can handle a request that contains 100 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "0.01",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m, accuracy\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtest_deploy_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[61], line 13\u001b[0m, in \u001b[0;36mtest_deploy_bert\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m ans \u001b[38;5;241m=\u001b[39m df_test_deploy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     12\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (predicted_ans \u001b[38;5;241m==\u001b[39m ans)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m, accuracy\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: 0.01"
     ]
    }
   ],
   "source": [
    "def test_deploy_bert():\n",
    "    df_test_deploy = df_squad.sample(100, random_state = 100)\n",
    "    input_json = json.dumps({\n",
    "        \"questions\" : df_test_deploy[\"question\"].tolist(),\n",
    "        \"context_paragraphs\" : df_test_deploy[\"context_paragraph\"].tolist()\n",
    "    })\n",
    "    response = requests.post(deployed_uri, input_json, headers = {'Content-Type' : 'application/json'})\n",
    "    check_equal(response.status_code, 200)\n",
    "    \n",
    "    predicted_ans = np.array(json.loads(response.content)[\"predicted_ans\"])\n",
    "    ans = df_test_deploy[\"answer\"].str.lower().values\n",
    "    accuracy = (predicted_ans == ans).mean()\n",
    "    assert accuracy >= 0.3, accuracy\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_deploy_bert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error 504 and the service log indicates a time out issue, you should look into optimizing the `run` function in `scoring.py`. You should not need to change anything in `get_bert_prediction`, but make sure to avoid repeated computations whenever possible (e.g., create the tokenizer in `init`, call `json.loads` only once)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's write the deployed URI to a text file so that it can be submitted to the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "with open('scoring_uri.txt', 'w') as f:\n",
    "    f.write(deployed_uri)\n",
    "    print(\"Saved scoring_uri to file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9: Clean up cloud resources**\n",
    "\n",
    "You have completed all the questions in this project! One last step you need to do is to make sure you clean up your resources properly, so that no unexpected charge is incurred. **You will still need to use Azure for the final exam**.\n",
    "\n",
    "Use the left navigation bar in the Azure Machine Learning Studio to manage your computes and endpoints. If you don't anticipate any further usage of Azure ML Studio for this project, you should go back to the [Azure homepage](https://portal.azure.com/) and delete the entire resource group, as the resource group itself also incurs charges over time, even without any computes or endpoints.\n",
    "\n",
    "<p style=\"color:red;\">\n",
    "    <strong>To check your remaining budget, you need to visit the <a href=\"https://portal.azure.com/#blade/Microsoft_Azure_Education/EducationMenuBlade/overview\">Education</a> tab on Azure. The Cost Mangement + Billing Tab will not display anything.</strong>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
