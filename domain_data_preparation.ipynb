{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Domain Data Preparation\n",
    "This project will help you familiarize with common data collection and preprocessing tasks. We will mine public text data related to the topic of coronavirus from different sources, and process and combine them to a single corpus, on which we can perform feature engineering to prepare for subsequent analyses.\n",
    "\n",
    "If you are running this notebook on Google Colab, use the provided code in the next section to set up your environment. If you are running this notebook locally, you can **delete the Environment setup section and start at the Package import section**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup (Google Colab only)\n",
    "After downloading and extracting the handout tar file, upload all of the handout files to Colab. Run the following cell to list all the file names in the current directory. Make sure you can see `domain_data_preparation.ipynb`, `local_test_refs`, `submitter`, `references`, `pdfs` and `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'domain_data_preparation.ipynb',\n",
       " 'local_test_refs',\n",
       " 'pdfs',\n",
       " 'references',\n",
       " 'requirements.txt',\n",
       " 'submitter']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now install all the Python packages needed for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.21.2 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from -r requirements.txt (line 1)) (1.21.2)\n",
      "Requirement already satisfied: pandas==1.3.3 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from -r requirements.txt (line 2)) (1.3.3)\n",
      "Requirement already satisfied: nltk==3.6.3 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from -r requirements.txt (line 3)) (3.6.3)\n",
      "Requirement already satisfied: beautifulsoup4==4.10.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from -r requirements.txt (line 4)) (4.10.0)\n",
      "Requirement already satisfied: selenium==3.141.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from -r requirements.txt (line 5)) (3.141.0)\n",
      "Requirement already satisfied: requests==2.27.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from -r requirements.txt (line 6)) (2.27.1)\n",
      "Requirement already satisfied: pdfminer.six==20201018 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from -r requirements.txt (line 7)) (20201018)\n",
      "Requirement already satisfied: jupyter==1.0.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from -r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from pandas==1.3.3->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from pandas==1.3.3->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: click in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nltk==3.6.3->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nltk==3.6.3->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: regex in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nltk==3.6.3->-r requirements.txt (line 3)) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nltk==3.6.3->-r requirements.txt (line 3)) (4.66.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from beautifulsoup4==4.10.0->-r requirements.txt (line 4)) (2.5)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from selenium==3.141.0->-r requirements.txt (line 5)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from requests==2.27.1->-r requirements.txt (line 6)) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from requests==2.27.1->-r requirements.txt (line 6)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from requests==2.27.1->-r requirements.txt (line 6)) (3.6)\n",
      "Requirement already satisfied: cryptography in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from pdfminer.six==20201018->-r requirements.txt (line 7)) (42.0.5)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from pdfminer.six==20201018->-r requirements.txt (line 7)) (2.4.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from pdfminer.six==20201018->-r requirements.txt (line 7)) (5.2.0)\n",
      "Requirement already satisfied: notebook in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (6.5.6)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (7.16.1)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (6.29.3)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 8)) (8.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from python-dateutil>=2.7.3->pandas==1.3.3->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from click->nltk==3.6.3->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from cryptography->pdfminer.six==20201018->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (8.18.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (7.4.9)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (5.7.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 8)) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 8)) (3.0.10)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (3.0.43)\n",
      "Requirement already satisfied: pygments in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (2.17.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (7.0.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (3.1.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (0.9.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (5.9.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (1.2.1)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (23.1.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (0.18.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (0.20.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 8)) (2.4.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (0.5.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from cffi>=1.12->cryptography->pdfminer.six==20201018->-r requirements.txt (line 7)) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from importlib-metadata>=3.6->nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (3.17.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (0.19.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (4.10.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (0.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (4.2.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (306)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (2.12.5)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (0.2.4)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (2.19.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (4.21.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 8)) (0.2.13)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (2.0.13)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (21.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (0.33.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->-r requirements.txt (line 8)) (0.18.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (4.3.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (0.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (0.5.2)\n",
      "Requirement already satisfied: overrides in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (7.7.0)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (1.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 8)) (0.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (0.1.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (2.4)\n",
      "Requirement already satisfied: uri-template in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (1.13)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r requirements.txt (line 8)) (2.8.19.20240106)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from matplotlib) (1.21.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from matplotlib) (6.1.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\iayus\\anaconda3\\envs\\p4\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the following code to get a Chrome webdriver in the current directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can import the packages and begin the project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iayus\\anaconda3\\envs\\P4\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json, collections, time, re, string, os\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "\n",
    "from pdfminer import high_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell has been tagged with excluded_from_script\n",
    "# it will not be run by the autograder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data on the internet is very messy.  Typically there is a fair amount of processing work to do once you have collected any sizeable chunk of text data, in order to have it ready for subsequent analyses. To get you familiar with this kind of data, this section will walk you through some common processing tasks:\n",
    "\n",
    "The first step is to import the lemmatizer and set of English stopwords from `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\", quiet = True)\n",
    "nltk.download(\"wordnet\", quiet = True)\n",
    "nltk.download(\"punkt\", quiet = True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet = True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "english_stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Text cleaning and tokenization\n",
    "Implement the three functions `clean_string`, `tokenize` and `lemmatize` that perform the following text preprocessing tasks:\n",
    "\n",
    "1. `clean_text` should:\n",
    "    * convert the string to lower case.\n",
    "    * remove any instance of `'s` that is either followed by any whitespace character, or at the end of the string: `teacher's help` becomes `teacher help`, and `children's` becomes `children`.\n",
    "    * remove apostrophe character `'`: `don't` becomes `dont`. For simplicity we will only consider the character `'` as apostrophe (so `‚Äô` is not).\n",
    "    * remove leading and trailing space.\n",
    "\n",
    "1. `tokenize` should:\n",
    "    * use `nltk.word_tokenize` to tokenize the input text.\n",
    "    * further break tokens at characters which are not digits 0-9 and not present in `string.ascii_letters`. For example, `a_b_c` becomes `['a', 'b', 'c']`.\n",
    "    * maintain the token order as it appears in the original string.\n",
    "\n",
    "1. `lemmatize` should:\n",
    "    * lemmatize each token individually.\n",
    "    * remove tokens that are stopwords or contain fewer than two characters (these two cases should be checked after the lemmatization step).\n",
    "    \n",
    "**Notes**:\n",
    "* When lemmatizing a word, you should also specify the part-of-speech `pos` parameter. This can be obtained by calling `nltk.pos_tag` and using the first returned tag (in case there are multiple possibilities). You can interpret the returned tag as follows:\n",
    "    * If it starts with \"J\", it is an adjective.\n",
    "    * If it starts with \"V\", it is a verb.\n",
    "    * If it starts with \"R\", it is an adverb.\n",
    "    * Otherwise, it is a noun.\n",
    "* `nltk.pos_tag` should be called on each individual token, instead of on the entire tokenized text. For example, if the input string is `\"learning is fun\"`, you should call `nltk.pos_tag([\"learning\"])` to get the part-of-speech of `'learning'`, and input that to the lemmatizer. You may notice that in this case `\"learning\"` is classified as a verb (while it is a noun in the original sentence). However, this is not a problem, since our end goal is to reduce each token to its base form, not to correctly classify its part-of-speech.\n",
    "* If you use the regex character set `\\w`, note that it matches alphanumeric characters **and** the underscore character `_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the input string by converting it to lowercase, removing 's and apostrophe.\n",
    "    \n",
    "    args:\n",
    "        text (str) : the input text\n",
    "        \n",
    "    return:\n",
    "        str : the cleaned text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"'s(\\s|$)\", r\"\\1\", text)\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(cleaned_text):\n",
    "    \"\"\"\n",
    "    Tokenize the input string.\n",
    "    \n",
    "    args:\n",
    "        cleaned_text (str): the input text, output from clean_text\n",
    "        \n",
    "    return:\n",
    "        List[str] : a list of tokens from the input text\n",
    "    \"\"\"\n",
    "    data = nltk.tokenize.word_tokenize(cleaned_text)\n",
    "    valid = set(string.ascii_letters + string.digits)\n",
    "    tokens = []\n",
    "    for token in data:\n",
    "        subtoken = ''\n",
    "        subtokens = []\n",
    "        for char in token:\n",
    "            if char in valid:\n",
    "                subtoken += char\n",
    "            else:\n",
    "                if subtoken:\n",
    "                    subtokens.append(subtoken)\n",
    "                    subtoken = ''\n",
    "        if subtoken:\n",
    "            subtokens.append(subtoken)\n",
    "        tokens.extend(subtokens)\n",
    "    return tokens\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convert treebank POS tags to wordnet POS tags.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize(tokens, stopwords = {}):\n",
    "    \"\"\"\n",
    "    Lemmatize each token in an input list of tokens\n",
    "    \n",
    "    args:\n",
    "        tokens (List[str]) : a list of token, output from tokenize\n",
    "    \n",
    "    kwargs:\n",
    "        stopwords (Set[str]) : the set of stopwords to exclude\n",
    "    \n",
    "    return:\n",
    "        List[str] : a list of lemmatized and filtered tokens\n",
    "    \"\"\"\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        pos_tag_list = nltk.pos_tag([token])\n",
    "        wordnet_pos = get_wordnet_pos(pos_tag_list[0][1])\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos=wordnet_pos)\n",
    "        if lemmatized_token not in stopwords and len(lemmatized_token) > 1:\n",
    "            lemmatized_tokens.append(lemmatized_token)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def preprocess_text(text, stopwords = {}):\n",
    "    # do not modify this function\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = tokenize(cleaned_text)\n",
    "    return lemmatize(tokens, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_preprocess_text():\n",
    "    # cleaning\n",
    "    assert clean_text(\"I like Data Science\") == \"i like data science\"\n",
    "    assert clean_text(\"She's\") == \"she\"\n",
    "    assert clean_text(\"you've\")== \"youve\"\n",
    "    assert clean_text(\"car, cars, car's cars'\")== \"car, cars, car cars\"\n",
    "    assert clean_text(\"'shed'\") == \"shed\"\n",
    "    assert clean_text(\"'good news'\") == \"good news\"\n",
    "    assert clean_text(\"CMU's campus\")== \"cmu campus\"\n",
    "    assert preprocess_text(\"abc 'system\") == ['abc', 'system']\n",
    "    assert preprocess_text(\"O'Shea Jackson Jr. is an American actor and musician\") == ['oshea', 'jackson', 'jr', 'be', 'an', 'american', 'actor', 'and', 'musician']\n",
    "    \n",
    "    # tokenization\n",
    "    assert tokenize(\"ab..ab. .ab . ab.\") == ['ab', 'ab', 'ab', 'ab'], tokenize(\"ab..ab. .ab . ab.\")\n",
    "    assert tokenize(\"word-of-mouth hello,world\")== ['word', 'of', 'mouth', 'hello', 'world']\n",
    "    assert tokenize(\"gotta\")== ['got', 'ta']\n",
    "    assert tokenize(\"hello_world\") == [\"hello\", \"world\"]\n",
    "    assert preprocess_text(\"hope thisüëèwill work\") == ['hope', 'this', 'will', 'work']\n",
    "    \n",
    "    # lemmatization\n",
    "    assert lemmatize([\"cats\"]) == ['cat']\n",
    "    assert lemmatize([\"did\"]) == ['do']\n",
    "    assert lemmatize([\"learning\", \"is\", \"fun\"], english_stopwords) == [\"learn\", \"fun\"]\n",
    "    \n",
    "    # miscellaneous\n",
    "    assert preprocess_text(\"the weather is really nice\", english_stopwords) == ['weather', 'really', 'nice']\n",
    "    assert preprocess_text(\n",
    "        \"To apply SVM learning in partial discharge classification, data input is very important!?\",\n",
    "        english_stopwords\n",
    "    ) == 'apply svm learn partial discharge classification data input important'.split()\n",
    "    assert preprocess_text(\"after all he's done\", english_stopwords) == []\n",
    "    assert preprocess_text(\"they didn‚Äôt have much chance of guessing what it was without further clues.\", english_stopwords) == ['much', 'chance', 'guess', 'without', 'far', 'clue']\n",
    "    assert preprocess_text(\"DUQUE'S\", english_stopwords) == [\"duque\"]\n",
    "    assert preprocess_text(\"the 'rona\", english_stopwords) == ['rona']\n",
    "    assert preprocess_text('MOTORCYCLES DONT FLY', english_stopwords)==['motorcycle', 'dont', 'fly']\n",
    "    assert preprocess_text('‚Äú Georg e\\‚Äù', english_stopwords) == ['georg']\n",
    "    text = \"Harry leapt into the air; he‚Äôd trodden on something big and squashy on the doormat ‚Äî something alive\"\n",
    "    assert preprocess_text(text, english_stopwords) == ['harry', 'leapt', 'air', 'trodden', 'something', 'big', 'squashy', 'doormat', 'something', 'alive']\n",
    "    assert preprocess_text(\"Don√¢‚Ç¨‚Ñ¢t want to add to TRUMP√¢‚Ç¨‚Ñ¢s #COVID19 numbers. #CoronaVirus √∞≈∏¬¶  don√¢‚Ç¨‚Ñ¢t care.\", english_stopwords) == ['want', 'add', 'trump', 'covid19', 'number', 'coronavirus', 'care']\n",
    "    \n",
    "    # test on long text string\n",
    "    with open(\"local_test_refs/henrys_letter.txt\", encoding = \"utf-8\") as infile, open(\"local_test_refs/processed_henrys_letter.txt\", encoding = \"utf-8\") as outfile:\n",
    "        processed_str = preprocess_text(infile.read())\n",
    "        reference_str = outfile.read().splitlines()\n",
    "        assert processed_str == reference_str\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_preprocess_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the lemmatization functionality isn't perfect; for example, it would map `\"as\"` to `\"a\"` because `\"as\"` is being treated as a noun instead of a proposition (with tag `\"IN\"`). In general, identifying the correct part-of-speech tag is very context-dependent (for example, `\"back\"` can be either an adjvective, adverb, verb or noun). In the context of this project, we will not dive deep into these linguistic nuances, and settle with the lemmatization rules above.\n",
    "\n",
    "The above processing function already covers a fair number of text cleaning tasks. We can now begin to collect data from online sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Tweet Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter is one of the most popular social media platforms; according to [Omnicore](https://www.omnicoreagency.com/twitter-statistics/#:~:text=There%20are%2048.35%20million%20monthly,monetizable%20daily%20active%20Twitter%20users.), it has 48.35 million active users, 42% of whom use Twitter on a daily basis. Furthermore, tweets are public by default, making the site a particularly rich data source on any given trending topic.\n",
    "\n",
    "In this section, you will extract tweets related to the topic of coronavirus. Due to the dynamic nature of Twitter, any user can edit or remove their old tweets, making it difficult to obtain deterministic results (and to autograde your code). Therefore, we will instead use a fixed Tweet [dataset from Kaggle](https://www.kaggle.com/smid80/coronavirus-covid19-tweets) and provide you with a custom API to query tweets from this dataset. For your own data science projects in the future, however, you are encouraged to explore the [official Twitter API](https://developer.twitter.com/en/docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Retrieve starting tweets\n",
    "Implement the function `get_tweets` that sends a GET request to https://gettweets.azurewebsites.net/11637/tweets and returns the status code as well as the response JSON. The response JSON is a list of dictionaries, each corresponding to one tweet and having the following format:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'text': 'hello world', # str, the tweet content\n",
    "    'lang': 'en', # str, the tweet language\n",
    "    'id': 123, # int, the tweet id\n",
    "    'time': '2019-12-04' # # str, yyyy-mm-dd\n",
    "}\n",
    "```\n",
    "\n",
    "**Notes**:\n",
    "* The API endpoint url https://gettweets.azurewebsites.net/11637/tweets is provided in the global variable `TWEET_API`.\n",
    "* You should call `.text` on the response object to retrieve its content, and then convert the content to JSON before returning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_API = \"https://gettweets.azurewebsites.net/11637/tweets\"\n",
    "\n",
    "def get_tweets(url = TWEET_API):\n",
    "    \"\"\"\n",
    "    Retrieve tweets by sending a GET request to the provided API endpoint.\n",
    "    \n",
    "    params:\n",
    "        url (str) : the url to send request to\n",
    "    \n",
    "    return:\n",
    "        Tuple(status_code, response):\n",
    "            status_code (int) : the response status code\n",
    "            response (str) : the response text\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    status_code = response.status_code\n",
    "    response_text = response.text\n",
    "    try:\n",
    "        response_json = json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        response_json = \"JSONDecodeError\"\n",
    "    return status_code, response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_get_tweets():\n",
    "    response_code, tweet_text = get_tweets()\n",
    "    tweet_jsons = json.loads(tweet_text)\n",
    "    assert response_code == 200\n",
    "    assert len(tweet_jsons) == 100\n",
    "    first10_tweet_ids = [tweet_json[\"id\"] for tweet_json in tweet_jsons[:10]]\n",
    "    assert first10_tweet_ids == [2819, 3075, 3331, 3587, 3843, 4099, 4355, 4611, 4867, 5123]\n",
    "    assert tweet_jsons[1][\"text\"] == 'Los Angeles County has identified 6 new cases of the coronavirus &amp; declared a local state of emergency.\\n#CoronavirusOutbreak #Coronavirusflorida #LosAngeles'\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_get_tweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, you now have 100 tweets at your disposal! Typically though, we would like to have more flexibility in our search, for example by specifying particular parameters that indicate our search objective. In this case, our API provides four parameters as follows:\n",
    "\n",
    "* `lang` - specify the tweet language, which is an [ISO 639-1 code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes), e.g., `en` for English.\n",
    "* `start` - the start date, formatted as yyyy-mm-dd; only tweets created on or after this date are returned.\n",
    "* `end` - the end date, formatted as yyyy-mm-dd; only tweets created on or before this date are returned.\n",
    "* `page` - the tweet page number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of the `page` parameter here. Because returning a large JSON would put burden on the server, we have restricted all response JSONs to only include 100 tweets. To get more tweets that also satisfy your search query, you can specify the `page` parameter based on the following formula: if `page = i` (indexed from 1) then the tweets from index `100*(i-1) + 1` to index `100*i` (inclusive) will be returned.\n",
    "\n",
    "As an example, if your configuration of `(lang, start, end)` yields 306 tweets, then:\n",
    "\n",
    "* `page = 1` or no `page` specified will return the tweets 1-100.\n",
    "* `page = 2` will return the tweets 101-200.\n",
    "* `page = 3` will return the tweets 201-300.\n",
    "* `page = 4` will return the tweets 301-306.\n",
    "* `page = 5` or larger will return a JSON with empty content `\"[]\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Search for tweets with parameters\n",
    "Implement the function `get_tweet_texts_with_params` that sends a GET request to the provided API endpoint with (optional) input parameters `lang`, `start`, `end` and `page`. This function should collect all tweets that satisfy the search query if the number of such tweets is smaller than a specified `n_tweets`, or the first `n_tweets` tweets otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_texts_with_params(url = TWEET_API, lang = \"all\", start = \"na\", end = \"na\", n_tweets = 10000):\n",
    "    \"\"\"\n",
    "    Search for tweets with parameters and extract their text content\n",
    "    \n",
    "    kwargs:\n",
    "        url (str) : the url to send request to\n",
    "        lang (str) : the tweet language in ISO 639-1 format\n",
    "        start (str) : the start date, yyyy-mm-dd\n",
    "        end (str) : the end date, yyyy-mm-dd\n",
    "        n_tweets (int) : the number of tweets to collect\n",
    "    \n",
    "    return:\n",
    "        List[str] : a list with the contents of the first n_tweets tweets returned from the search\n",
    "    \"\"\"\n",
    "    tweet_texts = []\n",
    "    page = 1\n",
    "    while len(tweet_texts) < n_tweets:\n",
    "        attr = {'lang': lang, 'start': start, 'end': end, 'page': page}\n",
    "        response = requests.get(url, params=attr)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        tweets = response.json()\n",
    "        if not tweets:\n",
    "            break\n",
    "        for tweet in tweets:\n",
    "            tweet_texts.append(tweet['text'])\n",
    "            if len(tweet_texts) >= n_tweets:\n",
    "                break\n",
    "        page += 1\n",
    "    return tweet_texts[:n_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_get_tweet_texts_with_params():\n",
    "    tweet_texts = get_tweet_texts_with_params(lang = \"en\", start = \"2020-03-09\", end = \"2020-03-09\")\n",
    "    assert len(tweet_texts) == 10000\n",
    "    example_tweets = [\n",
    "        '#coronavirus #COVID19 2 new cases reported in Kentucky (Harrison Co and Fayette Co), bring the states total to 6.',\n",
    "        \"#BREAKING\\n#Italy is extending its #coronavirus #quarantine measures, which include a ban on public gatherings, to the ENTIRE COUNTRY.\\nItaly's coronavirus death toll jumped on Monday by 97 to 463. \\nIt is the worst-hit country after #China.\\nhttps://t.co/Q6MNtyXttA\\n#COVID19\",\n",
    "        '#Investors√¢‚Ç¨‚Ñ¢ fortunes plunge as sell pressure hit banking stocks amid COVID-19 fear https://t.co/stihfvM7xZ via @MarketForcesA\\n\\n@Afrinvest @nsenigeria #equitymarket #fmcg #banks #insurance #oilandgas #COVID19',\n",
    "        'FROM A DOCTOR IN ITALY...\\n#coronavirus\\n#Coronavirusflorida\\n#CoronaVirusUpdates\\n#CoronavirusUSA\\n#COVID19 \\n#covid19Canada\\n#COVID19Toronto\\n#CoronaVirusCanada \\n#coronavirusToronto https://t.co/ZelZHWTuR1',\n",
    "        'Ina joint statement, Major League Baseball, Major League Soccer, the National Basketball Association, and the National Hockey League announced they are limiting locker room access due to concerns about the Coronavirus pandemic. #MLB #MLS #NBA #NHL #COVID19 #coronavirus https://t.co/Z6CUqxDrAO',\n",
    "        'Coronaviruses (CoV) are a large family of viruses that cause illness ranging from the common cold to more severe diseases such asMERS-CoVand SARS-CoV. A novel coronavirus (nCoV) is a new strain that has not been previously identified in humans.  #coronavirus #CoronavirusOutbreak',\n",
    "        'Simon Coveny and the entire Irish government attitude towards #covid19 just got destroyed on  #cblive',\n",
    "        'The reason for the toilet paper shortage is because when one person sneezes, 100 people shit themselves √∞≈∏¬§¬ß #coronavirus #COVID19 #tolietpaper',\n",
    "        'Latest #COVID19 numbers in Missouri - https://t.co/27vLQ3ZmqY',\n",
    "        'Tips on battling #CoronaVirusUpdate #COVID2019 #CoronavirusOutbreak \\nhttps://t.co/UaY8boPzHg'\n",
    "    ]\n",
    "    \n",
    "    assert tweet_texts[800:810] == example_tweets\n",
    "    emoji_tweet = \"Be carefull guy's and wish you all happy holi to you &amp; your family. :) \\n#HappyHoli #CoronavirusOutbreak #√†¬§¬π√†¬•‚Äπ√†¬§¬≤√†¬•‚Ç¨ #√†¬§¬π√†¬•‚Äπ√†¬§¬≤√†¬§¬ø√†¬§‚Ä¢√†¬§¬æ_√†¬§¬¶√†¬§¬π√†¬§¬® #BankLooteriBJP #Coronavid19 #marketcrash  #reliance #colours #KurkureWithSidNaaz #MondayMorning #MereAngneMein ##RangBarseWithSid #√†¬§¬¨√†¬•ÔøΩ√†¬§¬∞√†¬§¬æ_√†¬§¬®_√†¬§¬Æ√†¬§¬æ√†¬§¬®√†¬•‚Äπ_√†¬§¬π√†¬•‚Äπ√†¬§¬≤√†¬•‚Ç¨_√†¬§¬π√†¬•ÀÜ https://t.co/Rg2SpMNKZD\"\n",
    "    assert emoji_tweet in tweet_texts\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_get_tweet_texts_with_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Process tweet data\n",
    "Looking at some of the tweets above, we see that:\n",
    "1. Some tweets contain Twitter-shortened URLs, for example `https://t.co/DzhsXPxUDa`. These are always in the form of `http://t.co/` or `https://t.co/` followed by 10 alphanumeric characters. These links should be removed.\n",
    "1. Some tweets contain emoticons such as `:)` or `<3`. The characters in these emoticons should be removed.\n",
    "\n",
    "Implement the function `process_tweet` that takes as input a tweet text, performs the above two cleaning steps, and then calls `preprocess_text` on the cleaned tweet.\n",
    "\n",
    "**Notes**:\n",
    "* You should remove URL before removing emoticons.\n",
    "* We have provided a list of emoticons for you in the variable `emoticons`. You can assume that only elements in this set are considered emoticons and need to be removed. \n",
    "* Note that there may be no space between a shortened URL and the next word. However, you can assume that there are always 10 alphanumeric characters after http://t.co/ or https://t.co/.\n",
    "* Remember to specify the stopwords parameter `english_stopwords` when calling `preprocess_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons = [\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3',\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', 'b=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "]\n",
    "\n",
    "\n",
    "def process_tweet(tweet_text):\n",
    "    \"\"\"\n",
    "    Process and tokenize tweets, in addition to removing URLs and emoticons\n",
    "    \n",
    "    args:\n",
    "        tweet_text (str) : a list of tweet contents\n",
    "    \n",
    "    return:\n",
    "        List[str] :  a list of processed tokens from the input tweet\n",
    "    \"\"\"\n",
    "    tweet_text = re.sub(r'https?://t\\.co/[a-zA-Z0-9]{10}', ' ', tweet_text)\n",
    "    for emoticon in emoticons:\n",
    "        tweet_text = tweet_text.replace(emoticon, ' ')\n",
    "    tokens = preprocess_text(tweet_text, stopwords=english_stopwords)\n",
    "    return tokens\n",
    "\n",
    "# do not modify this function\n",
    "def process_tweet_data(tweet_texts):\n",
    "    return [process_tweet(tweet_text) for tweet_text in tweet_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_process_tweet():\n",
    "    assert process_tweet(\"It's a great day :D\") == ['great', 'day']\n",
    "    assert process_tweet(\"<3hello\") == [\"hello\"]\n",
    "    assert process_tweet(\"goodX-Dday\") == [\"good\", \"day\"]\n",
    "    assert process_tweet(\"http://t.co/WJs5bmRthU,http://t.co/WJs5bmRthU,\") == []\n",
    "    assert process_tweet(\"hellohttp://t.co/WJs5bmRthUworld\") == [\"hello\", \"world\"]\n",
    "    assert process_tweet(\"http://taco/WJs5bmRthU\") == ['http', 'taco', 'wjs5bmrthu']\n",
    "    assert process_tweet(\n",
    "        'Protect your child from #CoronavirusOutbreak.\\n\\nhttps://t.co/qPREVvM2C5\\n\\n#CoronaVirusUpdate #COVID2019 #COVID #Coronavid19 #outbreak #Italy #COVID√£∆í¬º19 #BeSafe #Containment #Homeschooling #DigitalTransformation #InternationalSchooling #virtualschool #OnlineNOW #edtech #technology'\n",
    "    ) == ['protect', 'child', 'coronavirusoutbreak', 'coronavirusupdate', 'covid2019', 'covid', 'coronavid19', 'outbreak', 'italy', 'covid', '19', 'besafe', 'containment', 'homeschooling', 'digitaltransformation', 'internationalschooling', 'virtualschool', 'onlinenow', 'edtech', 'technology']\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_process_tweet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move to the second method of data extraction: using Selenium and Beautifulsoup to parse HTML codes. More specifically, we will collect news articles related to the same topic of Coronavirus from two major media outlets -- [Nature](https://www.nature.com/) and [The New York Times](https://www.nytimes.com/). Through this exercise, you will learn how to navigate HTML structures from different webpages in order to get the desired information.\n",
    "\n",
    "To begin, we have provided you a helper function `retrieve_url` takes as input a webpage string URL and creates a BeautifulSoup object from the corresponding page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_url(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Parsing a single article from Nature\n",
    "Implement the function `parse_page_nature` that takes as input a URL string pointing to a Nature news article, and returns a JSON dictionary with the following format:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Title': 'When will the coronavirus outbreak peak?' #str\n",
    "    'Author': ['David Cyranoski'] # list, a list of author names in the same order as they appear on the page\n",
    "    'Published Date': '2020-04-21' # str, yyyy-mm-dd\n",
    "    'Summary': '.....' #str, the summary div between the title and author fields, or empty string if no summary is available\n",
    "    'Content': '.....' #list, the whole article content, where every element is a paragraph (i.e., comes from a <p> tag)\n",
    "}\n",
    "```\n",
    "\n",
    "The values of `Summary` and `Content` should be raw texts that do not contain any HTML tag. For example, if the input HTML code is `\"<p><b>Hello</b><a href=\"https://google.com\">World</a><p>\"` then the output `Content` should be `\"Hello World\"`.\n",
    "\n",
    "In the local test we have provided the full reference JSON files for some article pages. If your dictionary does not match the reference JSON, you should print out both and do a careful comparison to see where the difference is.\n",
    "\n",
    "**Notes**:\n",
    "* Occasionally there are some \"Related\" blocks embedded in the article text (example [here](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-domain-data-preparation/nature_related.png)). These are characterized by the attribute `data-label=\"Related\"` and should **not** be included in the parsing result.\n",
    "* The `Published Date` field should be the original article date, not the updated date. For example, the `Published Date` for [this article](http://web.archive.org/web/20210308142952/https://www.nature.com/articles/d41586-020-00166-6) is 2020-01-22.\n",
    "* Remember to call `strip()` on all values in the returned dictionary so that there is no leading or trailing space anywhere. If a content paragraph becomes empty after `strip()`, it should not be included. You do not need to call any other text processing task in section A.\n",
    "* Do not parse information form the `meta` tags as they are not robust. Every required information can be found within `body`.\n",
    "* If an article has no authors (e.g., [this article](http://web.archive.org/web/20201116084933/https://www.nature.com/articles/d41586-020-00589-1)), the Author field should be an empty list.\n",
    "* For the Content list, only text contents that come from the `p` tags in the article body should be included. You can start by identifying a `div` that corresponds to the entire article body (looking at the CSS class names may be helpful). Note that if an image caption is the child of a `p` tag, its content should be included as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page_nature(url):\n",
    "    \"\"\"\n",
    "    Parse a single New York Times article at the given URL\n",
    "    \n",
    "    args:\n",
    "        url (str) : the article URL\n",
    "    \n",
    "    return:\n",
    "        Dict[str, str] : the parsed information stored in JSON format, which includes:\n",
    "            Title, Author, Published Date, Summary and Content\n",
    "    \"\"\"\n",
    "    soup = retrieve_url(url)\n",
    "    result = {\"Title\": \"\", \"Author\": [], \"Published Date\": \"\", \"Summary\": \"\", \"Content\": []}\n",
    "    \n",
    "    title = soup.find('meta', attrs={'name': 'dc.title'})\n",
    "    if title:\n",
    "        result[\"Title\"] = re.sub(' +', ' ', title.get('content', '').strip())\n",
    "        \n",
    "    for author in soup.find_all('meta', attrs={'name': 'dc.creator'}):\n",
    "        author_name = re.sub(' +', ' ', author.get('content', '').strip())\n",
    "        if author_name:\n",
    "            result[\"Author\"].append(author_name)\n",
    "            \n",
    "    pub_date = soup.find('meta', attrs={'name': 'dc.date'})\n",
    "    if pub_date:\n",
    "        result[\"Published Date\"] = pub_date.get('content', '').strip()\n",
    "        \n",
    "    summary = soup.find('meta', attrs={'name': 'description'})\n",
    "    if summary:\n",
    "        result[\"Summary\"] = re.sub(' +', ' ', summary.get('content', '').strip())\n",
    "        \n",
    "    content_div = soup.select_one('div.article__body.cleared, div.article__body.serif.cleared')\n",
    "    if content_div:\n",
    "        for related in content_div.find_all(attrs={\"data-label\": \"Related\"}):\n",
    "            related.decompose()  \n",
    "\n",
    "        for child in content_div.children:\n",
    "            if child.name == 'p':\n",
    "                text = re.sub(' +', ' ', child.get_text().strip())\n",
    "                result[\"Content\"].append(text)\n",
    "            elif child.name == 'figure':\n",
    "                if not child.find(attrs={\"data-label\": \"Related\"}):\n",
    "                    figcaption = child.find('figcaption')\n",
    "                    if figcaption:\n",
    "                        caption_text = re.sub(' +', ' ', figcaption.get_text().strip())\n",
    "                        result[\"Content\"].append(caption_text)\n",
    "\n",
    "    result[\"Content\"] = [para for para in result[\"Content\"] if para]\n",
    "                    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_parse_page_nature():\n",
    "    nature1 = parse_page_nature(\"http://web.archive.org/web/20200430055159/https://www.nature.com/articles/d41586-020-00190-6\")\n",
    "    nature1_reference = json.load(open(\"local_test_refs/nature1.txt\"))\n",
    "    assert nature1 == nature1_reference, nature1\n",
    "    \n",
    "    nature2 = parse_page_nature(\"http://web.archive.org/web/20210308142952/https://www.nature.com/articles/d41586-020-00166-6\")\n",
    "    nature2_reference = json.load(open(\"local_test_refs/nature2.txt\"))\n",
    "    assert nature2 == nature2_reference, nature2\n",
    "    \n",
    "    nature3 = parse_page_nature(\"http://web.archive.org/web/20200604083109/https://www.nature.com/articles/d41586-020-00798-8\")\n",
    "    nature3_reference = json.load(open(\"local_test_refs/nature3.txt\"))\n",
    "    assert nature3 == nature3_reference, nature3\n",
    "    \n",
    "    nature4 = parse_page_nature(\"http://web.archive.org/web/20201116084933/https://www.nature.com/articles/d41586-020-00589-1\")\n",
    "    assert nature4[\"Author\"] == []\n",
    "    assert nature4[\"Published Date\"] == '2020-03-04'\n",
    "    assert len(nature4[\"Content\"]) == 26\n",
    "    \n",
    "    nature5 = parse_page_nature(\"http://web.archive.org/web/20200604071303/https://www.nature.com/articles/d41586-020-00786-y\")\n",
    "    assert nature5[\"Author\"] == ['Giuliana Viglione']\n",
    "    assert nature5[\"Published Date\"] == '2020-03-16'\n",
    "    assert len(nature5[\"Content\"]) == 14\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_parse_page_nature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Parsing several Nature articles from a search page\n",
    "Now that you have successfully parsed individual article pages, the next step is to search for all the relevant articles and collect their titles. More specifically, we want to search for articles that:\n",
    "1. contain the term \"coronavirus\" (case-insensitive) *in their titles*\n",
    "1. do not contain any of the terms \"Daily briefing\", \"Podcast\" or \"Backchat\" (case-insensitive) in their titles\n",
    "1. were published in a given period of time (e.g., February 01, 2020 to March 01, 2020 inclusive)\n",
    "1. have \"News\" as the Article type and belong to the journal \"Nature\" -- these criteria can be specified in the search result page.\n",
    "\n",
    "Explore the [Nature search page](https://www.nature.com/search) and [Advanced search page](https://www.nature.com/search/advanced) to see how you may obtain the desired search results. Then implement the function `extract_nature_articles` that returns a list of titles for articles that meet the search criteria.\n",
    "\n",
    "**Notes**:\n",
    "* You do not need Selenium for this question. Pay attention to how the search parameters are reflected in the URL.\n",
    "* The article titles should be ordered based on their associated dates **on the search result page**, from earlier to later. If two articles have the same date, order them alphabetically based on their titles.\n",
    "* If Nature's search functionalities do not support all of the search criteria, you can implement your own filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nature_articles(start_date, end_date, base_url = \"https://www.nature.com\"):\n",
    "    \"\"\"\n",
    "    Search for and parse all coronavirus-related News article from the Nature journal that were\n",
    "    published in a given period\n",
    "    \n",
    "    args:\n",
    "        start_date (str): the lower bound of the date range to filter articles,\n",
    "            has the format yyyy-mm-dd\n",
    "        end_date (str): the upper bound (inclusive) of the date range to filter articles,\n",
    "            has the format yyyy-mm-dd\n",
    "    \n",
    "    kwargs:\n",
    "        base_url (str): the home page url of Nature\n",
    "    \n",
    "    return:\n",
    "        List[str] : a list of article titles that meet the search criteria, ordered by\n",
    "            date and by title\n",
    "    \"\"\"\n",
    "    start_year = start_date[:4]\n",
    "    end_year = end_date[:4]\n",
    "    query_url = f\"{base_url}/search?q=coronavirus&order=date_desc&journal=nature&article_type=news&date_range={start_year}-{end_year}&title=coronavirus\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "    data = []\n",
    "    in_range = True\n",
    "    page = 1\n",
    "    while in_range:\n",
    "        url = f\"{query_url}&page={page}\"\n",
    "        content = retrieve_url(url)\n",
    "        articles = content.find_all('article')\n",
    "        if not articles:\n",
    "            break\n",
    "        for article in articles:\n",
    "            title = article.find('h3', {'class': 'c-card__title'})\n",
    "            publish_date = article.find('time', {'class': 'c-meta__item'})\n",
    "            if title and publish_date:\n",
    "                article_title = title.get_text(strip=True)\n",
    "                publish_date = datetime.strptime(publish_date['datetime'], \"%Y-%m-%d\").date()\n",
    "                if start <= publish_date <= end and \"coronavirus\" in article_title.lower():\n",
    "                    exclude = [\"Daily briefing\", \"Podcast\", \"Backchat\"]\n",
    "                    if all(term.lower() not in article_title.lower() for term in exclude):\n",
    "                        data.append((article_title, publish_date))\n",
    "                else:\n",
    "                    if publish_date < start:\n",
    "                        in_range = False\n",
    "                        break\n",
    "        page += 1\n",
    "    data.sort(key=lambda x: (x[1], x[0]))\n",
    "    final_titles = [title for title, _ in data]\n",
    "    return final_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_extract_nature_articles():\n",
    "    parsed_titles = extract_nature_articles(\"2020-02-01\", \"2020-03-01\")\n",
    "    expected_titles = [\n",
    "        'HIV vaccine failure, coronavirus papers and an unprecedented glimpse of the Sun',\n",
    "        'Did pangolins spread the China coronavirus to people?',\n",
    "        'How scientists are fighting the novel coronavirus: A three minute guide',\n",
    "        'CRISPR enhancement, coronavirus source and a controversial appointment',\n",
    "        'Scientists fear coronavirus spread in countries least able to contain it',\n",
    "        'More than 80 clinical trials launch to test coronavirus treatments',\n",
    "        'When will the coronavirus outbreak peak?',\n",
    "        'Coronavirus name, animal-research data and a Solar System snowman',\n",
    "        'Scientists question China‚Äôs decision not to report symptom-free coronavirus cases',\n",
    "        'China set to clamp down permanently on wildlife trade in wake of coronavirus',\n",
    "        '‚ÄòNo one is allowed to go out‚Äô: your stories from the coronavirus outbreak',\n",
    "        'Time to use the p-word? Coronavirus enters dangerous new phase',\n",
    "        'Mystery deepens over animal source of coronavirus'\n",
    "    ]\n",
    "    expected_dates = [\n",
    "        '2020-02-05', '2020-02-07', '2020-02-07', '2020-02-12',\n",
    "        '2020-02-13', '2020-02-15', '2020-02-18', '2020-02-19',\n",
    "        '2020-02-20', '2020-02-21', '2020-02-21', '2020-02-25', '2020-02-26'\n",
    "    ]\n",
    "    \n",
    "    assert len(parsed_titles) == 13\n",
    "    assert parsed_titles == expected_titles, parsed_titles\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_extract_nature_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce bias in our data, it is typically a good idea to collect data from more than one source. Below are two **optional** challenges that involve the same web scraping process but on a different news site -- New York Times. Because it has a completely distinct HTML structure from Nature, you will need to inspect the webpage to identify the tags that contain the required information. The second optional challenge (Question 8) will also involve using Selenium to dynamically collect the search result. If you like to test your web scraping skill, give these questions a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Parsing a single article from the New York Times (Optional)\n",
    "\n",
    " **This is an optional, ungraded question. Feel free to proceed to Question 9 and revisit this later if you are interested.** \n",
    "\n",
    "Implement the function `parse_page_nyt` that takes as input an NYT article string URL and returns a JSON dictionary with the following format:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Title': 'F.D.A. Approves First Coronavirus Antibody Test in U.S.' #str\n",
    "    'Author': ['Katie Thomas', 'Natasha Singer'] # list, a list of author names in the same order as they appear on the page\n",
    "    'Published Date': '2020-04-21' # str, yyyy-mm-dd\n",
    "    'Summary': '.....' #str, the summary paragraph between the title and author fields, or empty string if no summary is available\n",
    "    'Content': '.....' #list, the whole article content, where every element is a paragraph (i.e., comes from a <p> tag)\n",
    "}\n",
    "```\n",
    "\n",
    "The values of `Summary` and `Content` should be raw text that do not contain any HTML tag. For example, if the input HTML code is `\"<p><b>Hello</b> <a href=\"https://google.com\">World</a><p>\"` then the output should be `\"Hello World\"`.\n",
    "\n",
    "In the local test we have provided the full reference JSON files for some article pages. If your dictionary does not match the reference JSON, you should print out both and do a careful comparison to see where the difference is.\n",
    "\n",
    "**Notes**:\n",
    "* Ignore the \"Frequently Asked Questions and Advice\" box (example [here](http://clouddatascience.blob.core.windows.net/m20-foundation-data-science/p3-domain-data-preparation/nyt_faq.png)).\n",
    "* The summary can be extracted from a `<p>` tag whose id is `article-summary`. If this tag is not present, simply use the empty string as the value.\n",
    "* For the `Content` field, you only need to find the `<p>` tags inside the `<div>` tags that have the class `StoryBodyCompanionColumn`.\n",
    "* You may find unicode characters, e.g., `\\u201d`, while parsing the page. It is fine to include them in the output. \n",
    "* Remember to call `strip()` on all values in the returned dictionary so that there is no leading or trailing space anywhere. If a content paragraph becomes empty after `strip()`, it should not be included. You do not need to call any other text processing task in section A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page_nyt(url):\n",
    "    \"\"\"\n",
    "    Parse a New York Times article page to extract the title, authors, date, summary and content.\n",
    "    \n",
    "    args:\n",
    "        url (str) : the article page's URL\n",
    "    \n",
    "    return:\n",
    "        Dict[str, str] : a JSON-like dictionary whose keys are \"Title\", \"Author\", \"Published Date\", \"Summary\" and \"Content\"\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_parse_page_nyt():\n",
    "    nyt1 = parse_page_nyt(\"https://web.archive.org/web/20200603034948/https://www.nytimes.com/2020/04/21/health/fda-in-home-test-coronavirus.html\")\n",
    "    nyt1_reference = json.load(open(\"local_test_refs/nyt1.txt\"))\n",
    "    assert nyt1 == nyt1_reference\n",
    "    \n",
    "    nyt2 = parse_page_nyt(\"https://web.archive.org/web/20200602022004/https://www.nytimes.com/2020/04/18/health/kidney-dialysis-coronavirus.html\")\n",
    "    nyt2_reference = json.load(open(\"local_test_refs/nyt2.txt\"))\n",
    "    assert nyt2 == nyt2_reference\n",
    "    \n",
    "    nyt3 = parse_page_nyt(\"https://web.archive.org/web/20200520224040/https://www.nytimes.com/2020/03/23/health/medical-students-coronavirus.html\")\n",
    "    nyt3_reference = json.load(open(\"local_test_refs/nyt3.txt\"))\n",
    "    assert nyt3 == nyt3_reference\n",
    "    \n",
    "    nyt4 = parse_page_nyt(\"https://web.archive.org/web/20200626171621/https://www.nytimes.com/2020/03/23/health/medical-students-coronavirus.html\")\n",
    "    assert nyt4[\"Author\"] == ['Penina Krieger', 'Abby Goodnough']\n",
    "    assert nyt4[\"Published Date\"] == \"2020-03-23\"\n",
    "    assert len(nyt4[\"Content\"]) == 27\n",
    "    \n",
    "    nyt5 = parse_page_nyt(\"https://web.archive.org/web/20200809005447/https://www.nytimes.com/2020/03/20/health/coronavirus-data-logarithm-chart.html\")\n",
    "    assert nyt5[\"Author\"] == [\"Kenneth Chang\"]\n",
    "    assert nyt5[\"Published Date\"] == \"2020-03-20\"\n",
    "    assert len(nyt5[\"Content\"]) == 8\n",
    "\n",
    "    nyt6 = parse_page_nyt(\"https://web.archive.org/web/20200710083045/https://www.nytimes.com/2020/03/20/health/coronavirus-italy-men-risk.html\")\n",
    "    assert nyt6[\"Author\"] == ['Roni Caryn Rabin']\n",
    "    assert nyt6[\"Published Date\"] == \"2020-03-20\"\n",
    "    assert len(nyt6[\"Content\"]) == 17\n",
    "    \n",
    "    nyt7 = parse_page_nyt(\"https://web.archive.org/web/20200522153110/https://www.nytimes.com/2020/03/20/health/coronavirus-nurses-healthcare.html\")\n",
    "    assert nyt7[\"Author\"] == ['Emma Goldberg']\n",
    "    assert nyt7[\"Published Date\"] == \"2020-03-20\"\n",
    "    assert len(nyt7[\"Content\"]) == 20\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_parse_page_nyt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Parsing several NYT article titles from the search page (optional)\n",
    "\n",
    " **This is an optional, ungraded question. Feel free to proceed to Question 9 and revisit this later if you are interested.** \n",
    " \n",
    "Just like with Nature, we can browse the [NYT search page](https://www.nytimes.com/search) and programmatically collect all the search results, which can then be parsed individually using `parse_page_nyt`.\n",
    "\n",
    "A caveat here is that the Nature search page partitions the search results into several webpages, each with a unique URL, while NYT includes all the search results in one webpage. Initially only the first 10 results are visible; we need to click on the \"SHOW MORE\" button to see the next 10 results, then click again for the next 10, and so on. Naturally, this task is best suited for Selenium.\n",
    "\n",
    "We provide the Selenium code to extract the search results here. This code will search for articles that:\n",
    "\n",
    "1. are presented by entering the term \"coronavirus\" in the search box.\n",
    "1. were published in a given period of time (e.g., February 01, 2020 to March 01, 2020 inclusive)\n",
    "1. have \"Article\" as the type and belong to the \"Health\" section  -- these criteria can be specified in the search result page.\n",
    "\n",
    "\n",
    "To run this code you will need to have downloaded either the Chromedriver or Geckodriver, depending on your preferred browser (see the [Data Collection and Extraction Primer](https://nbviewer.jupyter.org/url/clouddatascience.blob.core.windows.net/primers/data-collection-extraction-primer/data_collection_extraction_primer.ipynb) for detailed installation instructions). Then, edit the `USE_GECKODRIVER` variable, and fill in either the `PATH_TO_CHROMEDRIVER` or `PATH_TO_GECKODRIVER` variable below, then execute the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "# set this to True if you are on Colab or using a Chromium browser, such as Google Chrome\n",
    "# set this to False if you are using Firefox locally\n",
    "USE_CHROMEDRIVER = True\n",
    "\n",
    "# path to the chromedriver executable, edit this if you use chromedriver locally\n",
    "# if you are on Colab, do not change this path\n",
    "# if you are on Windows, you may need to add .exe to the end of the path\n",
    "PATH_TO_CHROMEDRIVER = \"./chromedriver\"\n",
    "\n",
    "# path to the geckodriver executable, edit this if you use geckodriver locally\n",
    "# if you are on Windows, you may need to add .exe to the end of the path\n",
    "PATH_TO_GECKODRIVER = \"./geckodriver\"\n",
    "\n",
    "# do not modify this function\n",
    "def init_geckodriver(debug = False):\n",
    "    options = webdriver.FirefoxOptions()\n",
    "    if not debug:\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument(\"--disable-setuid-sandbox\")\n",
    "        options.add_argument('--user-agent=\"\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36\"\"')\n",
    "    return webdriver.Firefox(executable_path = PATH_TO_GECKODRIVER, options = options)\n",
    "\n",
    "# do not modify this function\n",
    "def init_chromedriver(debug = False):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if not debug:\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument(\"--disable-setuid-sandbox\")\n",
    "        options.add_argument('--user-agent=\"\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36\"\"')\n",
    "    return webdriver.Chrome(executable_path = PATH_TO_CHROMEDRIVER, options = options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nyt_article_titles(start_date, end_date, base_url = \"https://www.nytimes.com\"):\n",
    "    \"\"\"\n",
    "    Search for and parse all coronavirus-related News article from the New York Times that were\n",
    "    published in a given period\n",
    "    \n",
    "    args:\n",
    "        start_date (str): the lower bound of the date range to filter articles,\n",
    "            has the format yyyy-mm-dd\n",
    "        end_date (str): the upper bound (inclusive) of the date range to filter articles,\n",
    "            has the format yyyy-mm-dd\n",
    "    \n",
    "    kwargs:\n",
    "        base_url (str): the home page url of New York Times\n",
    "    \n",
    "    return:\n",
    "        List[Dict[str, str]] : a list of parsed JSON for each articles returned by\n",
    "            the search query\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "# print all the search result titles from the following page:\n",
    "# https://www.nytimes.com/search?dropmab=true&endDate=20210301&query=coronavirus&sections=Health%7Cnyt%3A%2F%2Fsection%2F9f943015-a899-5505-8730-6d30ed861520&sort=oldest&startDate=20210201&types=article\n",
    "extract_nyt_article_titles(\"2021-02-01\", \"2021-03-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Process news articles data\n",
    "While the JSON data format we constructed earlier is useful for checking the correctness of our parsing, eventually we would like each article to be represented by just a string, one that we can input to `preprocess_text` and get a list of processed tokens. For our purpose, we will define the string representation of an article as\n",
    "\n",
    "`\"<title> <summary> <content paragraph 1> <content paragraph 2> <content paragraph 3> ...\"`\n",
    "\n",
    "where there is a single space separating each field (note that the content paragraphs come from the `\"Content\"` field of an article json, which is a list of paragraph strings).\n",
    "\n",
    "Implement the function `process_news_article` that takes as input a JSON dictionary resulting from parsing a Nature or NYT article, converts the JSON to the above string format, and outputs a list of processed tokens from the article string.\n",
    "\n",
    "**Note**:\n",
    "* Remember to specify the stopwords parameter as `english_stopwords` when you call `preprocess_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news_article(article_json):\n",
    "    \"\"\"\n",
    "    Convert article jsons to nested list of tokens of processed article contents\n",
    "    \n",
    "    args:\n",
    "        article_json (Dict[str, str]] : JSON content of a news article\n",
    "    \n",
    "    return:\n",
    "        List[str] : a list of processed tokens from the input article JSON\n",
    "    \"\"\"\n",
    "    elements = [article_json['Title'], article_json['Summary']] + article_json['Content']\n",
    "    article_string = ' '.join(elements)\n",
    "    processed_tokens = preprocess_text(article_string, stopwords=english_stopwords)\n",
    "    return processed_tokens\n",
    "\n",
    "# do not modify this function\n",
    "def process_news_articles_data(article_jsons):\n",
    "    return [process_news_article(article) for article in article_jsons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_process_news_article():\n",
    "    nature_article = json.load(open(\"local_test_refs/nature1.txt\"))   \n",
    "    nature_article_processed = process_news_article(nature_article)\n",
    "    nature_expected = open(\"local_test_refs/nature1_processed.txt\").read().splitlines()\n",
    "    assert nature_article_processed == nature_expected\n",
    "    \n",
    "    nyt_article = json.load(open(\"local_test_refs/nyt1.txt\"))\n",
    "    nyt_article_processed = process_news_article(nyt_article)\n",
    "    nyt_expected = open(\"local_test_refs/nyt1_processed.txt\").read().splitlines()\n",
    "    assert nyt_article_processed == nyt_expected\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_process_news_article()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Mining PDF Data\n",
    "Having extracted data from Twitter and newspapers, we now turn to our third source: research papers. We have provided you with 15 pdf files, collected from the [arxiv API](https://arxiv.org/help/api). These are located in the `pdfs` directory and labeled from `arxiv_01.pdf` to `arxiv_15.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: Parse a single Arxiv research paper\n",
    "Implement the function `parse_pdf` that takes as input a PDF file path and outputs the processed tokenization of the text content of that file. In particular, you should perform the following steps:\n",
    "\n",
    "1. Remove all URLs, i.e., strings that start with \"http://\" or \"https://\"\n",
    "1. Call the `preprocess_text` function you implemented in part A. Remember to specify the `stopwords` parameter.\n",
    "\n",
    "**Notes**:\n",
    "* For this question, you should use the function [`extract_text`](https://pdfminersix.readthedocs.io/en/latest/reference/highlevel.html#extract-text) from the `pdfminer` package to convert a pdf file to string.\n",
    "* Unlike in the tweet scenario, there is no limit on the length of an URL in this case. The URL pattern you should use here is: a string that starts with `http://` or `https://`, followed by any number of non-space character. Do not make any other assumption (for example, don't assume an URL always contains a `.`).\n",
    "* We have provided a template helper function `remove_url_regex`, where you can enter the regex for removing URLs. There are some local test cases in `test_remove_url_regex` to help you validate your regex. If your regex passes these tests, you can use it in `parse_and_clean_pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_regex():\n",
    "    # enter your regex for capturing url strings here\n",
    "    # you can call this function in parse_and_clean_pdf()\n",
    "    regex = r'https?://\\S+'\n",
    "    return regex\n",
    "\n",
    "def parse_and_clean_pdf(file):\n",
    "    \"\"\"\n",
    "    Convert an input pdf file into processed and cleaned raw text.\n",
    "    \n",
    "    args:\n",
    "        file (str) : the pdf file path\n",
    "    \n",
    "    return:\n",
    "        List[str] : the cleaned tokenization of the input file content\n",
    "    \"\"\"\n",
    "    raw_text = high_level.extract_text(file)\n",
    "    url_regex = remove_url_regex()\n",
    "    text_without_urls = re.sub(url_regex, '', raw_text)\n",
    "    text = preprocess_text(text_without_urls, stopwords=english_stopwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_remove_url_regex():\n",
    "    s = \"http://abc\"\n",
    "    assert re.sub(remove_url_regex(), '', s) == ''\n",
    "    \n",
    "    s = 'hellohttps://github.com/lanagarmire/COVID19-Drugs-LungInjury'\n",
    "    assert re.sub(remove_url_regex(), '', s) == 'hello'\n",
    "    \n",
    "    s = 'http://example.com https://cmu.edu'\n",
    "    assert re.sub(remove_url_regex(), '', s) == ' '\n",
    "    \n",
    "    s = 'https://www.'\n",
    "    assert re.sub(remove_url_regex(), '', s) == ''\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_remove_url_regex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_parse_and_clean_pdf():\n",
    "    pdf_text = parse_and_clean_pdf(\"pdfs/arxiv_01.pdf\")\n",
    "    with open(\"local_test_refs/parsed_arxiv_01.txt\") as outfile:\n",
    "        assert pdf_text == outfile.read().splitlines()\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_parse_and_clean_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: Parse several Arxiv research papers\n",
    "Implement the function `process_arxiv_data` that takes as input the path to a directory. This function parses and cleans all pdf files in that directory, then returns a nested list of word tokens, where each inner list results from parsing one PDF file.\n",
    "\n",
    "**Notes**:\n",
    "* The pdf files should be processed based on the alphabetical order of their name, e.g., `arxiv_01.pdf` before `arxiv_02.pdf`.\n",
    "* Do not assume that `os.listdir` will return the filenames in sorted order; you should perform the sorting yourself.\n",
    "* Do not assume every file in the input directory is a pdf file; only those whose names end in `.pdf` should be parsed.\n",
    "* If you fail the test case here, it is likely that your URL removal regex from Question 10 is incorrect. Try to come up with more test cases to test your URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arxiv_data(directory):\n",
    "    \"\"\"\n",
    "    Parse and process the text content of all pdf papers in alphabetical order in a given directory\n",
    "    \n",
    "    args:\n",
    "        directory (str) : the relative file path to a directory that contains the pdf papers\n",
    "    \n",
    "    return:\n",
    "        List[List[str]] : a list of list of word tokens\n",
    "    \"\"\"\n",
    "    files = os.listdir(directory)\n",
    "    pdf_files = sorted([f for f in files if f.lower().endswith('.pdf')])\n",
    "    all_tokens = []\n",
    "    for pdf_file in pdf_files:\n",
    "        file_path = os.path.join(directory, pdf_file)\n",
    "        tokens = parse_and_clean_pdf(file_path)\n",
    "        all_tokens.append(tokens)\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_process_arxiv_data():\n",
    "    paper_contents = process_arxiv_data(\"pdfs\")\n",
    "    first_words = [paper[0] for paper in paper_contents]\n",
    "    mid_words = [paper[100] for paper in paper_contents]\n",
    "    last_words = [paper[-1] for paper in paper_contents]\n",
    "    \n",
    "    assert len(paper_contents) == 15\n",
    "    assert first_words == [\n",
    "        'repurposed', 'fractal', 'coronavirus', 'data', 'xu1', 'parametric',\n",
    "        'view', 'insight', 'reconstruction', 'covid', 'outbreak', 'scale',\n",
    "        'abnormal', 'trend', 'deep'\n",
    "    ]\n",
    "    assert mid_words == [\n",
    "        'result', 'nal', 'ro', 'february', 'covid', 'sars',\n",
    "        'virus', 'export', 'system', 'new', 'transmission', 'cantly',\n",
    "        'signi', 'content', 'wenling'\n",
    "    ]\n",
    "    assert last_words == [\n",
    "        '2019', '13', '122567', '13', 'lancet', 'url',\n",
    "        '77', 'hour', '330', '16', 'provide', '2004',\n",
    "        '212', '2020', '29'\n",
    "    ]\n",
    "    assert sum(len(paper) for paper in paper_contents) == 35969\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_process_arxiv_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E: Data Visualization and Feature Construction\n",
    "Now that we have collected text data from three different sources (Twitter, news articles and research papers), let's put them all together in order to perform some simple exploratory data analyses and feature construction. From now we will define a *document* as a list of tokens coming from a single tweet, news article or arxiv paper, and a *corpus* as a list of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12: Word frequency and word cloud\n",
    "With any text corpus, you will first want to check for the word frequency distribution, in particular which words are the most common and which are the least. The former group may consist of terms that are relevant to the topic, or terms that simply appear frequently in general (e.g., stopwords). The latter group may consist of highly specialized terms or typos. Since stopwords and rare words are not useful to our analysis, we will remove both (where we define rare words as words that only appear *once in the corpus*).\n",
    "\n",
    "Implement the function `word_frequency` which takes as input a text corpus and returns a `collections.Counter` object mapping each word to its frequency in the corpus. However, rare words that only appear once in the entire corpus should **not** be included in this mapping.\n",
    "\n",
    "**Notes**:\n",
    "* Recall that `preprocess_text` already handles stopword removal, so you only need to remove rare words in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(corpus):\n",
    "    \"\"\"\n",
    "    Count the word frequency in a given corpus\n",
    "    \n",
    "    args:\n",
    "        corpus (List[List[str]]) : a nested list of tokens, where each inner list is a processed document\n",
    "    \n",
    "    return:\n",
    "        collections.Counter : a mapping between each word and its frequency in the corpus, excluding words that\n",
    "            only appear once\n",
    "    \"\"\"\n",
    "    words = [word for document in corpus for word in document]\n",
    "    word_counts = collections.Counter(words)\n",
    "    filtered_word_counts = collections.Counter({word: count for word, count in word_counts.items() if count > 1})\n",
    "    return filtered_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_word_frequency():\n",
    "    tweet_corpus = process_tweet_data(\n",
    "        get_tweet_texts_with_params(lang = \"en\", start = \"2020-03-09\", end = \"2020-03-09\", n_tweets = 1000)\n",
    "    )\n",
    "    counter = word_frequency(tweet_corpus)\n",
    "    assert len(counter) == 1739\n",
    "    assert counter[\"coronavirus\"] == 407\n",
    "    assert counter[\"coronavirusoutbreak\"] == 684\n",
    "    assert counter[\"increasingly\"] == 2\n",
    "    assert counter[\"stimulus\"] == 3\n",
    "    assert min(counter.values()) == 2\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_word_frequency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will gather all three corpuses together; we store them in a global cache to avoid having to construct them more than once. If you make any code change above this point, rerun the following cell to reset the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuses = None\n",
    "\n",
    "def get_corpuses():\n",
    "    global corpuses\n",
    "    if corpuses is None:\n",
    "        twitter_corpus = process_tweet_data(\n",
    "            get_tweet_texts_with_params(lang = \"en\", start = \"2020-03-09\", end = \"2020-03-09\")\n",
    "        )\n",
    "        news_data_files = [\n",
    "            \"nature1.txt\", \"nature2.txt\", \"nature3.txt\",\n",
    "            \"nyt1.txt\", \"nyt2.txt\", \"nyt3.txt\"\n",
    "        ]\n",
    "        news_corpus = process_news_articles_data([\n",
    "            json.load(open(f\"local_test_refs/{filename}\"))\n",
    "            for filename in news_data_files\n",
    "        ])\n",
    "        arxiv_corpus = process_arxiv_data(\"pdfs\")\n",
    "        corpuses = (twitter_corpus, news_corpus, arxiv_corpus)\n",
    "    return corpuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first compare the frequency of a number of keywords across these three corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "# this cell has been taggged with excluded_from_script\n",
    "# it will not be run by the autograder\n",
    "def get_word_frequency_across_corpuses(input_words):    \n",
    "    twitter_corpus, news_corpus, arxiv_corpus = get_corpuses()\n",
    "    twitter_corpus_size = sum(len(d) for d in twitter_corpus)\n",
    "    news_corpus_size = sum(len(d) for d in news_corpus)\n",
    "    arxiv_corpus_size = sum(len(d) for d in arxiv_corpus)\n",
    "    twitter_f, news_f, arxiv_f = word_frequency(twitter_corpus), word_frequency(news_corpus), word_frequency(arxiv_corpus)\n",
    "    return pd.DataFrame({\n",
    "        \"Proportion in twitter corpus\" : [twitter_f.get(word, 0) / twitter_corpus_size for word in input_words],\n",
    "        \"Proportion in news corpus\" : [news_f.get(word, 0) / news_corpus_size for word in input_words],\n",
    "        \"Proportion in arxiv corpus\" : [arxiv_f.get(word, 0) / arxiv_corpus_size for word in input_words]\n",
    "    }, index = input_words)\n",
    "\n",
    "df_frequency = get_word_frequency_across_corpuses([\n",
    "    \"coronavirus\", \"covid\", \"case\", \"health\", \"model\", \"say\", \"test\",\n",
    "    \"2020\", \"19\", \"people\", \"vaccine\"\n",
    "])\n",
    "\n",
    "display(df_frequency)\n",
    "\n",
    "df_frequency.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are differences across datasets in the relative frequency of each term. \"Coronavirus\" is used most frequently in tweets, \"say\" most frequently in news corpus, and perhaps unsurprisingly, \"model\" most frequently in arxiv papers. The scientific notation of coronavirus, \"covid,\" isn't used in news articles as much, but is equally popular in both tweets and arxiv papers. On the other hand, \"health\" sees most frequent usage in news articles, likely due to health advice-related articles. Feel free to edit the word list above and see what other insights you can derive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move to the last step of data collection and preparation: constructing input features to be used for more formal analyses and language modeling. As language modeling will be covered later in the course, here we will only cover two simple feature construction methods: term frequency (TF) and term frequency - inverse document frequency (TF-IDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13: Feature construction: term frequency (TF)\n",
    "Implement the function `construct_tf_matrix` that takes as input a corpus and outputs a matrix $TF$ where each row corresponds to one document, and each column corresponds to one of the unique words in the entire corpus. $TF_{ij}$ is the number of times word $j$ appears in document $i$. Similar to the previous question, rare words that only appear once in the entire corpus should be removed, i.e., there should be no columns for those words.\n",
    "\n",
    "**Notes**:\n",
    "* The rows should be ordered based on the document ordering in the corpus. Row 0 corresponds to `corpus[0]`, row 1 to `corpus[1]`, and so on.\n",
    "* The columns should be ordered based on the alphabetical order of their corresponding words. Column 0 corresponds to the alphabetically first word in the corpus, column 1 to the alphabetically second word, and so on.\n",
    "* To ensure code efficiency, avoid using too many loops. Take advantage of Pandas and Numpy functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tf_matrix(corpus):\n",
    "    \"\"\"\n",
    "    Construct a term frequency matrix from an input corpus\n",
    "    \n",
    "    args:\n",
    "        corpus (List[List[str]]) : a nested list of word tokens, where each inner list is a document\n",
    "    \n",
    "    return:\n",
    "        np.array[n_documents, n_words] : the term frequency matrix\n",
    "    \"\"\"\n",
    "    words = word_frequency(corpus)\n",
    "    words = sorted(list(set(words)))\n",
    "    tf_matrix = pd.DataFrame(0, index=np.arange(len(corpus)), columns=words)\n",
    "    for i, document in enumerate(corpus):\n",
    "        for word in document:\n",
    "            if word in tf_matrix.columns:\n",
    "                tf_matrix.loc[i, word] += 1\n",
    "    return tf_matrix.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_construct_tf_matrix():\n",
    "    corpus = [\n",
    "        \"this project is project 4 in foundations of computational data science\".split(),\n",
    "        \"it covers text data collection and preparation in the data science pipeline\".split(),\n",
    "        \"text processing can be tricky sometimes\".split()\n",
    "    ]\n",
    "    tf = construct_tf_matrix(corpus)\n",
    "    assert (tf == np.array([\n",
    "        [1, 1, 2, 1, 0],\n",
    "        [2, 1, 0, 1, 1],\n",
    "        [0, 0, 0, 0, 1]]\n",
    "    )).all()\n",
    "    \n",
    "    twitter_corpus, news_corpus, arxiv_corpus = get_corpuses()\n",
    "    all_corpuses = twitter_corpus + news_corpus + arxiv_corpus\n",
    "    tf = construct_tf_matrix(all_corpuses)\n",
    "    assert tf.dtype == np.int64\n",
    "    assert tf.shape == (10021, 9871), tf.shape\n",
    "    assert (tf.sum(axis = 1)[:10] == np.array([9, 14, 10, 17, 27, 4, 22, 23, 17, 13])).all(), tf.sum(axis = 1)[:10]\n",
    "    assert (tf.sum(axis = 1)[-10:] == np.array([1465, 1935, 4681, 1857, 2093, 2496, 1993, 2324, 1809, 3369])).all(), tf.sum(axis = 1)[-10:]\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_construct_tf_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14: Feature construction: term frequency - inverse document frequency (TF-IDF)\n",
    "We can now compute the TF-IDF matrix, which scales the columns of the term frequency matrix by their inverse document frequency. Recall that the inverse document frequency of a word $j$ is computed as\n",
    "$$\\text{IDF}_j = \\log \\left( \\frac{\\# \\text{ of documents}}{\\# \\text{ of documents with word } j} \\right),$$\n",
    "and so the $\\text{TF-IDF}_{ij}$ entry in the tf-idf matrix is computed as\n",
    "$$\\text{TF-IDF}_{ij} = \\text{TF}_{ij} \\times \\text{IDF}_j.$$\n",
    "\n",
    "Implement the function `tf_idf_matrix` which takes as input a TF matrix and outputs the corresponding TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tf_idf_matrix(tf_matrix):\n",
    "    \"\"\"\n",
    "    Compute the term frequency - inverse document frequency in a corpus\n",
    "    \n",
    "    args:\n",
    "        tf_matrix (np.array[n_documents, n_words]) : the term frequency document of the corpus\n",
    "    \n",
    "    return:\n",
    "        np.array[n_documents, n_words] : the tf-idf matrix\n",
    "    \"\"\"\n",
    "    df = np.count_nonzero(tf_matrix, axis=0)\n",
    "    no_of_documents = tf_matrix.shape[0]\n",
    "    idf = np.log(no_of_documents/(df))\n",
    "    matrix = tf_matrix * idf\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "excluded_from_script"
    ]
   },
   "outputs": [],
   "source": [
    "def test_construct_tf_idf_matrix():\n",
    "    corpus = [\n",
    "        \"this project is project 4 in foundations of computational data science\".split(),\n",
    "        \"it covers text data collection and preparation in the data science pipeline\".split(),\n",
    "        \"text processing can be tricky sometimes\".split()\n",
    "    ]\n",
    "    tf_idf = construct_tf_idf_matrix(construct_tf_matrix(corpus))\n",
    "    assert np.allclose(tf_idf, np.array([\n",
    "        [0.40546511, 0.40546511, 2.19722458, 0.40546511, 0.        ],\n",
    "        [0.81093022, 0.40546511, 0.        , 0.40546511, 0.40546511],\n",
    "        [0.        , 0.        , 0.        , 0.        , 0.40546511]\n",
    "    ]))\n",
    "    print(\"All tests passed!\")\n",
    "    \n",
    "test_construct_tf_idf_matrix()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
